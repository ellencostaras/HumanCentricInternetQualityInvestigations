{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_null_timestamps(stats_dict):\n",
    "\t'''Function to find the roughly 30 second range of timestamps which Web RTC consistently provides data for.'''\n",
    "\n",
    "\tglobal_leftmost = -1\n",
    "\tglobal_rightmost = 99999999999\n",
    "\tfor key, val in stats_dict.items():\n",
    "\t\tleftmost = None\n",
    "\t\trightmost = None\n",
    "\t\tfor i in range(len(val)):\n",
    "\t\t\tif val[i] != -1 and val[i] != \"-1\":\n",
    "\t\t\t\tif leftmost == None:\n",
    "\t\t\t\t\tleftmost = i\n",
    "\t\t\t\t\trightmost = i\n",
    "\t\t\t\tif rightmost != None:\n",
    "\t\t\t\t\trightmost = i\n",
    "\t\t\telif (val[i] == -1 or val[i] == \"-1\") and rightmost != None:\n",
    "\t\t\t\tbreak\n",
    "\t\tif leftmost > global_leftmost:\n",
    "\t\t\tglobal_leftmost = leftmost\n",
    "\t\tif rightmost < global_rightmost:\n",
    "\t\t\tglobal_rightmost = rightmost\n",
    "\n",
    "\treturn global_leftmost + 1, global_rightmost\n",
    "\n",
    "\n",
    "def aggregate_across_call_single_stat(stat_list, percentile, leftmost, rightmost):\n",
    "\t'''function to aggregate Web RTC stat for a whole call into one'''\n",
    "\t\n",
    "\tif rightmost - leftmost <= 0:\n",
    "\t\traise ValueError(\"Error: Statistic found for which Web RTC recorded no data for.\") \n",
    "\telse:\n",
    "\t\tstat_list_reduced = np.array(stat_list[leftmost : rightmost + 1])\n",
    "\t\tagg_value = np.percentile(stat_list_reduced, percentile)\n",
    "\t\treturn agg_value\n",
    "\n",
    "\n",
    "def find_agg_stats_single_call(parent_file_path, treatment):\n",
    "\t'''function which outputs a list of aggregated statistics (one for every\n",
    "    Web RTC stat parsed), across the call'''\n",
    "\t\n",
    "\tfile_path = parent_file_path + str(treatment) + \".csv\"\n",
    "\tdf = pd.read_csv(file_path) #read in as pandas data frame\n",
    "\tleftmost, rightmost = find_non_null_timestamps(df.to_dict(orient='list'))\n",
    "\t\n",
    "\tsmall_is_bad = [\n",
    "\t\t          \"IT01V_packetsRecieved_ellen\",\n",
    "\t\t\t      \"IT01V_packetsRecieved_aadya\",\n",
    "\t\t\t\t  \"IT01V_frameWidth_ellen\",\n",
    "\t\t\t\t  \"IT01V_frameWidth_aadya\",\n",
    "\t\t\t\t  \"IT01V_frameHeight_ellen\",\n",
    "\t\t\t\t  \"IT01V_frameHeight_aadya\",\n",
    "\t\t\t\t  \"IT01V_framesPerSecond_ellen\",\n",
    "\t\t\t\t  \"IT01V_framesPerSecond_aadya\",\n",
    "\t\t\t\t  \"IT01V_bytesReceived_in_bits/s_ellen\",\n",
    "\t\t\t\t  \"IT01V_bytesReceived_in_bits/s_aadya\",\n",
    "\t\t\t\t  \"IT01A_bytesReceived_in_bits/s_ellen\",\n",
    "\t\t\t\t  \"IT01A_bytesReceived_in_bits/s_aadya\",\n",
    "\t\t\t\t  \"OT01V_packetsSent/s_ellen\",\n",
    "\t\t\t\t  \"OT01V_packetsSent/s_aadya\",\n",
    "\t\t\t\t  \"OT01V_bytesSent_in_bits/s_ellen\",\n",
    "\t\t\t\t  \"OT01V_bytesSent_in_bits/s_aadya\",\n",
    "\t\t\t\t  \"OT01V_frameWidth_ellen\",\n",
    "\t\t\t\t  \"OT01V_frameWidth_aadya\",\n",
    "\t\t\t\t  \"OT01V_framesPerSecond_ellen\",\n",
    "\t\t\t\t  \"OT01V_framesPerSecond_aadya\",\n",
    "\t\t\t\t  ]\n",
    "\tbig_is_bad = [\n",
    "\t\t\t\t  \"IT01V_packetsLost_ellen\", \n",
    "\t\t\t\t  \"IT01V_packetsLost_aadya\",\n",
    "\t\t\t\t  \"IT01V_totalFreezesDuration_ellen\",\n",
    "\t\t\t\t  \"IT01V_totalFreezesDuration_aadya\",\n",
    "\t\t\t\t  \"IT01V_totalProcessingDelay_ellen\",\n",
    "\t\t\t\t  \"IT01V_totalProcessingDelay_aadya\",\n",
    "\t\t\t\t  \"IT01V_jitter_ellen\",\n",
    "\t\t\t\t  \"IT01V_jitter_aadya\",\n",
    "\t\t\t\t  \"IT01V_jitterBufferDelay/emissions_ellen\",\n",
    "\t\t\t\t  \"IT01V_jitterBufferDelay/emissions_aadya\",\n",
    "\t\t\t\t  \"IT01A_jitterBufferDelay/emissions_ellen\",\n",
    "\t\t\t\t  \"IT01A_jitterBufferDelay/emissions_aadya\",\n",
    "\t\t\t\t  \"OT01V_totalPacketSendDelay_ellen\",\n",
    "\t\t\t\t  \"OT01V_totalPacketSendDelay_aadya\",\n",
    "\t\t\t\t  \"OT01V_totalPacketSendDelay/packetsSent_in_ms_ellen\",\n",
    "\t\t\t\t  \"OT01V_totalPacketSendDelay/packetsSent_in_ms_aadya\",\n",
    "\t\t\t\t  \"RIV_roundTripTime_ellen\",\n",
    "\t\t\t\t  \"RIV_roundTripTime_aadya\",\n",
    "\t\t\t\t  \"RIV_fractionLost_ellen\",\n",
    "\t\t\t\t  \"RIV_fractionLost_aadya\",\n",
    "\t\t\t\t  \"RIA_fractionLost_ellen\",\n",
    "\t\t\t\t  \"RIA_fractionLost_aadya\",\n",
    "\t\t\t\t  \"RIA_roundTripTime_ellen\",\n",
    "\t\t\t\t  \"RIA_roundTripTime_aadya\",\n",
    "\t\t\t\t  \"ROA_roundTripTime_ellen\",\n",
    "\t\t\t\t  \"ROA_roundTripTime_aadya\",\n",
    "\t\t\t\t  \"AP_totalPlayoutDelay_ellen\",\n",
    "\t\t\t\t  \"AP_totalPlayoutDelay_aadya\"]\n",
    "                  \n",
    "\tagg_values = []\n",
    "\tfor col_name, col_data in df.iteritems():\n",
    "\t\tif col_name in small_is_bad:\n",
    "\t\t\tpercentile = 10 \n",
    "\t\t\tagg_values.append(aggregate_across_call_single_stat(col_data, percentile, leftmost, rightmost))\n",
    "\t\telif col_name in big_is_bad:\n",
    "\t\t\tpercentile = 90\n",
    "\t\t\tagg_values.append(aggregate_across_call_single_stat(col_data, percentile, leftmost, rightmost))\n",
    "\t\t\t\n",
    "\treturn agg_values\n",
    "\t\t\n",
    "\n",
    "def create_regression_table(readin_parent_file_path, writeout_file_path, lowest_treatment_number, highest_treatment_number):\n",
    "\t\n",
    "    header = [\n",
    "\t\t\"call_number\",\n",
    "\t\t\"IT01V_packetsRecieved_ellen\",\n",
    "        \"IT01V_packetsRecieved_aadya\",\n",
    "\t\t\"IT01V_packetsLost_ellen\", \n",
    "        \"IT01V_packetsLost_aadya\",\n",
    "        \"IT01V_frameWidth_ellen\",\n",
    "        \"IT01V_frameWidth_aadya\",\n",
    "        \"IT01V_frameHeight_ellen\",\n",
    "        \"IT01V_frameHeight_aadya\",\n",
    "\t\t\"IT01V_totalFreezesDuration_ellen\",\n",
    "        \"IT01V_totalFreezesDuration_aadya\",\n",
    "        \"IT01V_framesPerSecond_ellen\",\n",
    "        \"IT01V_framesPerSecond_aadya\",\n",
    "        \"IT01V_bytesReceived_in_bits_s_ellen\",\n",
    "        \"IT01V_bytesReceived_in_bits_s_aadya\",\n",
    "\t\t\"IT01V_totalProcessingDelay_ellen\",\n",
    "        \"IT01V_totalProcessingDelay_aadya\",\n",
    "        \"IT01V_jitter_ellen\",\n",
    "        \"IT01V_jitter_aadya\",\n",
    "        \"IT01V_jitterBufferDelay_emissions_ellen\",\n",
    "        \"IT01V_jitterBufferDelay_emissions_aadya\",\n",
    "        \"IT01A_bytesReceived_in_bits_s_ellen\",\n",
    "        \"IT01A_bytesReceived_in_bits_s_aadya\",\n",
    "\t\t\"IT01A_jitterBufferDelay_emissions_ellen\",\n",
    "        \"IT01A_jitterBufferDelay_emissions_aadya\",\n",
    "        \"OT01V_packetsSent_s_ellen\",\n",
    "        \"OT01V_packetsSent_s_aadya\",\n",
    "        \"OT01V_bytesSent_in_bits_s_ellen\",\n",
    "        \"OT01V_bytesSent_in_bits_s_aadya\",\n",
    "        \"OT01V_frameWidth_ellen\",\n",
    "        \"OT01V_frameWidth_aadya\",\n",
    "        \"OT01V_framesPerSecond_ellen\",\n",
    "        \"OT01V_framesPerSecond_aadya\",\n",
    "\t\t\"OT01V_totalPacketSendDelay_ellen\",\n",
    "        \"OT01V_totalPacketSendDelay_aadya\",\n",
    "        \"OT01V_totalPacketSendDelay_packetsSent_in_ms_ellen\",\n",
    "        \"OT01V_totalPacketSendDelay_packetsSent_in_ms_aadya\",\n",
    "        \"RIV_roundTripTime_ellen\",\n",
    "        \"RIV_roundTripTime_aadya\",\n",
    "        \"RIV_fractionLost_ellen\",\n",
    "        \"RIV_fractionLost_aadya\",\n",
    "        \"RIA_fractionLost_ellen\",\n",
    "        \"RIA_fractionLost_aadya\",\n",
    "        \"RIA_roundTripTime_ellen\",\n",
    "        \"RIA_roundTripTime_aadya\",\n",
    "        \"ROA_roundTripTime_ellen\",\n",
    "        \"ROA_roundTripTime_aadya\",\n",
    "        \"AP_totalPlayoutDelay_ellen\",\n",
    "        \"AP_totalPlayoutDelay_aadya\"\n",
    "    ]\n",
    "\n",
    "\t# Write out to a CSV\n",
    "    with open(writeout_file_path, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        for treatment in tqdm(range(lowest_treatment_number, highest_treatment_number + 1)):\n",
    "            try:\n",
    "                row = [treatment] + find_agg_stats_single_call(readin_parent_file_path, treatment)\n",
    "                writer.writerow(row)\n",
    "            except Exception as E:\n",
    "                print(f\"WARNING: unable to generate aggregate statistics for treatment {treatment} due to ...\")\n",
    "                print(\"      \", E)\n",
    "\n",
    "\n",
    "def combine_tables(csv1_path, csv2_path, output_csv_path):\n",
    "    '''function to simply merge two tables into one baed on the name in the first column being the same'''\n",
    "\t# Read both CSV files into pandas dataframes\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "    # Rename the first column to 'identifier' in both dataframes if not already named\n",
    "    df1.rename(columns={df1.columns[0]: 'identifier'}, inplace=True)\n",
    "    df2.rename(columns={df2.columns[0]: 'identifier'}, inplace=True)\n",
    "\n",
    "    # Merge the two dataframes on the 'identifier' column using an inner join\n",
    "    merged_df = pd.merge(df1, df2, on='identifier', how='inner')\n",
    "\n",
    "    # Write the merged dataframe to a new CSV file\n",
    "    merged_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 262/300 [00:03<00:00, 59.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 254 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 65.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# STAGE 1\n",
    "\n",
    "readin_parent_file_path = \"../parsed_CSVs/stage_1/treatment\"\n",
    "writeout_file_path = \"independent_vars_tables/stage_1_independent_vars_table.csv\"\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 300\n",
    "create_regression_table(readin_parent_file_path, writeout_file_path, lowest_treatment_number, highest_treatment_number)\n",
    "\n",
    "#merging with treatment conditions:\n",
    "treatment_csv_filepath = \"../test_combos/stage_1/test_combos_shuffled_aadya.csv\"\n",
    "output_file_path = \"regression_tables/stage_1_regression_table.csv\"\n",
    "combine_tables(writeout_file_path, treatment_csv_filepath, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [00:00<00:03, 67.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 33 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 189/300 [00:02<00:01, 71.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 178 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for treatment 184 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for treatment 187 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 235/300 [00:03<00:00, 70.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 224 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for treatment 226 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 265/300 [00:03<00:00, 68.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 256 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_2/treatment256.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 66.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 298 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STAGE 2\n",
    "\n",
    "readin_parent_file_path = \"../parsed_CSVs/stage_2/treatment\"\n",
    "writeout_file_path = \"independent_vars_tables/stage_2_independent_vars_table.csv\"\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 300\n",
    "create_regression_table(readin_parent_file_path, writeout_file_path, lowest_treatment_number, highest_treatment_number)\n",
    "\n",
    "#merging with treatment conditions:\n",
    "treatment_csv_filepath = \"../test_combos/stage_2/test_combos_shuffled_aadya.csv\"\n",
    "output_file_path = \"regression_tables/stage_2_regression_table.csv\"\n",
    "combine_tables(writeout_file_path, treatment_csv_filepath, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 14/300 [00:00<00:04, 69.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 1 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment1.csv'\n",
      "WARNING: unable to generate aggregate statistics for treatment 5 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment5.csv'\n",
      "WARNING: unable to generate aggregate statistics for treatment 11 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment11.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 188/300 [00:02<00:01, 73.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 180 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment180.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 261/300 [00:03<00:00, 73.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 249 due to ...\n",
      "       Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for treatment 252 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment252.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 285/300 [00:04<00:00, 72.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for treatment 272 due to ...\n",
      "       [Errno 2] No such file or directory: '../parsed_CSVs/stage_3/treatment272.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 69.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# STAGE 3\n",
    "\n",
    "readin_parent_file_path = \"../parsed_CSVs/stage_3/treatment\"\n",
    "writeout_file_path = \"independent_vars_tables/stage_3_independent_vars_table.csv\"\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 300\n",
    "create_regression_table(readin_parent_file_path, writeout_file_path, lowest_treatment_number, highest_treatment_number)\n",
    "\n",
    "#merging with treatment conditions:\n",
    "treatment_csv_filepath_1 = \"../test_combos/stage_3/test_combos_shuffled_aadya.csv\"\n",
    "treatment_csv_filepath_2 = \"../test_combos/stage_3/test_combos_shuffled_ellen.csv\"\n",
    "output_file_path = \"regression_tables/stage_3_regression_table.csv\"\n",
    "combine_tables(writeout_file_path, treatment_csv_filepath_1, output_file_path)\n",
    "combine_tables(output_file_path, treatment_csv_filepath_2, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For fieldwork stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: unable to generate aggregate statistics for colac11 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/colac/colac1/test1.csv'\n",
      "WARNING: unable to generate aggregate statistics for colac13 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/colac/colac1/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for colac21 due to...\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for colac22 due to...\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for colac32 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/colac/colac3/test2.csv'\n",
      "WARNING: unable to generate aggregate statistics for colac33 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/colac/colac3/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for dunkeld11 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/dunkeld/dunkeld1/test1.csv'\n",
      "WARNING: unable to generate aggregate statistics for dunkeld13 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/dunkeld/dunkeld1/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for dunkeld21 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/dunkeld/dunkeld2/test1.csv'\n",
      "WARNING: unable to generate aggregate statistics for dunkeld22 due to...\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for dunkeld32 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/dunkeld/dunkeld3/test2.csv'\n",
      "WARNING: unable to generate aggregate statistics for ararat33 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/ararat/ararat3/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for bendigo31 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/bendigo/bendigo3/test1.csv'\n",
      "WARNING: unable to generate aggregate statistics for elmore33 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/elmore/elmore3/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for myrtleford31 due to...\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for myrtleford33 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/myrtleford/myrtleford3/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for euroa33 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/euroa/euroa3/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for seymore22 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/seymore/seymore2/test2.csv'\n",
      "WARNING: unable to generate aggregate statistics for seymore23 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/seymore/seymore2/test3.csv'\n",
      "WARNING: unable to generate aggregate statistics for seymore32 due to...\n",
      "        [Errno 2] No such file or directory: '../parsed_CSVs/fieldwork/seymore/seymore3/test2.csv'\n",
      "WARNING: unable to generate aggregate statistics for mordialloc1\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n",
      "WARNING: unable to generate aggregate statistics for mordialloc2\n",
      "        Error: Statistic found for which Web RTC recorded no data for.\n"
     ]
    }
   ],
   "source": [
    "# Fieldwork\n",
    "\n",
    "rural_town_names = [\"colac\", \"dunkeld\", \"ararat\", \"bendigo\", \"elmore\", \"shep\", \"wang\", \"myrtleford\", \"euroa\", \"seymore\"]\n",
    "urban_suburb_names = [\"dandenong\", \"mordialloc\", \"brighton\", \"toorak\", \"cbd\", \"brunswickwest\", \"northcote\"]\n",
    "\n",
    "writeout_file_path = \"independent_vars_tables/fieldwork_independent_vars_table.csv\"\n",
    "\n",
    "header = [\n",
    "\t\t\"ID\",\n",
    "\t\t\"IT01V_packetsRecieved_ellen\",\n",
    "        \"IT01V_packetsRecieved_aadya\",\n",
    "\t\t\"IT01V_packetsLost_ellen\", \n",
    "        \"IT01V_packetsLost_aadya\",\n",
    "        \"IT01V_frameWidth_ellen\",\n",
    "        \"IT01V_frameWidth_aadya\",\n",
    "        \"IT01V_frameHeight_ellen\",\n",
    "        \"IT01V_frameHeight_aadya\",\n",
    "\t\t\"IT01V_totalFreezesDuration_ellen\",\n",
    "        \"IT01V_totalFreezesDuration_aadya\",\n",
    "        \"IT01V_framesPerSecond_ellen\",\n",
    "        \"IT01V_framesPerSecond_aadya\",\n",
    "        \"IT01V_bytesReceived_in_bits_s_ellen\",\n",
    "        \"IT01V_bytesReceived_in_bits_s_aadya\",\n",
    "\t\t\"IT01V_totalProcessingDelay_ellen\",\n",
    "        \"IT01V_totalProcessingDelay_aadya\",\n",
    "        \"IT01V_jitter_ellen\",\n",
    "        \"IT01V_jitter_aadya\",\n",
    "        \"IT01V_jitterBufferDelay_emissions_ellen\",\n",
    "        \"IT01V_jitterBufferDelay_emissions_aadya\",\n",
    "        \"IT01A_bytesReceived_in_bits_s_ellen\",\n",
    "        \"IT01A_bytesReceived_in_bits_s_aadya\",\n",
    "\t\t\"IT01A_jitterBufferDelay_emissions_ellen\",\n",
    "        \"IT01A_jitterBufferDelay_emissions_aadya\",\n",
    "        \"OT01V_packetsSent_s_ellen\",\n",
    "        \"OT01V_packetsSent_s_aadya\",\n",
    "        \"OT01V_bytesSent_in_bits_s_ellen\",\n",
    "        \"OT01V_bytesSent_in_bits_s_aadya\",\n",
    "        \"OT01V_frameWidth_ellen\",\n",
    "        \"OT01V_frameWidth_aadya\",\n",
    "        \"OT01V_framesPerSecond_ellen\",\n",
    "        \"OT01V_framesPerSecond_aadya\",\n",
    "\t\t\"OT01V_totalPacketSendDelay_ellen\",\n",
    "        \"OT01V_totalPacketSendDelay_aadya\",\n",
    "        \"OT01V_totalPacketSendDelay_packetsSent_in_ms_ellen\",\n",
    "        \"OT01V_totalPacketSendDelay_packetsSent_in_ms_aadya\",\n",
    "        \"RIV_roundTripTime_ellen\",\n",
    "        \"RIV_roundTripTime_aadya\",\n",
    "        \"RIV_fractionLost_ellen\",\n",
    "        \"RIV_fractionLost_aadya\",\n",
    "        \"RIA_fractionLost_ellen\",\n",
    "        \"RIA_fractionLost_aadya\",\n",
    "        \"RIA_roundTripTime_ellen\",\n",
    "        \"RIA_roundTripTime_aadya\",\n",
    "        \"ROA_roundTripTime_ellen\",\n",
    "        \"ROA_roundTripTime_aadya\",\n",
    "        \"AP_totalPlayoutDelay_ellen\",\n",
    "        \"AP_totalPlayoutDelay_aadya\"\n",
    "    ]\n",
    "\n",
    "with open(writeout_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for town in rural_town_names:\n",
    "        for location in range(1, 4):\n",
    "            readin_parent_file_path = f\"../parsed_CSVs/fieldwork/{town}/{town}{location}/test\"\n",
    "            for test in range(1, 5):\n",
    "                try:\n",
    "                    row = find_agg_stats_single_call(readin_parent_file_path, test)\n",
    "                    writer.writerow([f\"{town}{location}{test}\"] + row)\n",
    "                except Exception as E:\n",
    "                    if test != 4:\n",
    "                        print(f\"WARNING: unable to generate aggregate statistics for {town}{location}{test} due to...\")\n",
    "                        print(\"       \", E)\n",
    "    \n",
    "    for suburb in urban_suburb_names:\n",
    "        readin_parent_file_path = f\"../parsed_CSVs/fieldwork/{suburb}/test\"\n",
    "        for test in range(1, 4):\n",
    "            try:\n",
    "                row = find_agg_stats_single_call(readin_parent_file_path, test)\n",
    "                writer.writerow([f\"{suburb}{test}\"] + row)\n",
    "            except Exception as E:\n",
    "                print(f\"WARNING: unable to generate aggregate statistics for {suburb}{test}\")\n",
    "                print(\"       \", E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_vars_file_path = \"independent_vars_tables/fieldwork_independent_vars_table.csv\"\n",
    "subjective_file_path = \"../test_combos/fieldwork/fieldwork_subjective.csv\"\n",
    "writeout_file_path = \"regression_tables/fieldwork_regression_table.csv\"\n",
    "combine_tables(independent_vars_file_path, subjective_file_path, writeout_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random testing/checking functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2 0\n",
      "3 6\n",
      "4 3\n",
      "5 0\n",
      "6 6\n",
      "7 1\n",
      "8 5\n",
      "9 0\n",
      "10 0\n",
      "11 4\n",
      "12 0\n",
      "13 1\n",
      "14 1\n",
      "15 0\n",
      "16 2\n",
      "17 8\n",
      "18 6\n",
      "19 0\n",
      "20 0\n",
      "21 0\n",
      "22 2\n",
      "23 5\n",
      "24 0\n",
      "25 0\n",
      "26 1\n",
      "27 0\n",
      "28 2\n",
      "29 1\n",
      "30 8\n",
      "31 0\n",
      "32 2\n",
      "33 4\n",
      "34 0\n",
      "35 0\n",
      "36 0\n",
      "37 0\n",
      "38 0\n",
      "39 0\n",
      "40 1\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 2\n",
      "46 8\n",
      "47 0\n",
      "48 2\n",
      "49 2\n",
      "50 1\n",
      "51 9\n",
      "52 0\n",
      "53 0\n",
      "54 0\n",
      "55 0\n",
      "56 2\n",
      "57 2\n",
      "58 0\n",
      "59 0\n",
      "60 1\n",
      "61 0\n",
      "62 0\n",
      "63 1\n",
      "64 0\n",
      "65 0\n",
      "66 1\n",
      "67 0\n",
      "68 0\n",
      "69 1\n",
      "70 0\n",
      "71 2\n",
      "72 0\n",
      "73 0\n",
      "74 2\n",
      "75 2\n",
      "76 0\n",
      "77 0\n",
      "78 0\n",
      "79 4\n",
      "80 0\n",
      "81 2\n",
      "82 2\n",
      "83 0\n",
      "84 5\n",
      "85 0\n",
      "86 0\n",
      "87 0\n",
      "88 2\n",
      "89 0\n",
      "90 0\n",
      "91 1\n",
      "92 0\n",
      "93 2\n",
      "94 2\n",
      "95 0\n",
      "96 2\n",
      "97 1\n",
      "98 0\n",
      "99 3\n",
      "100 0\n",
      "101 1\n",
      "102 0\n",
      "103 2\n",
      "104 3\n",
      "105 0\n",
      "106 1\n",
      "107 0\n",
      "108 0\n",
      "109 2\n",
      "110 4\n",
      "111 2\n",
      "112 0\n",
      "113 1\n",
      "114 5\n",
      "115 0\n",
      "116 1\n",
      "117 0\n",
      "118 0\n",
      "119 5\n",
      "120 0\n",
      "121 1\n",
      "122 0\n",
      "123 3\n",
      "124 0\n",
      "125 0\n",
      "126 0\n",
      "127 0\n",
      "128 0\n",
      "129 2\n",
      "130 1\n",
      "131 7\n",
      "132 0\n",
      "133 2\n",
      "134 1\n",
      "135 5\n",
      "136 0\n",
      "137 0\n",
      "138 2\n",
      "139 0\n",
      "140 3\n",
      "141 0\n",
      "142 3\n",
      "143 0\n",
      "144 0\n",
      "145 4\n",
      "146 0\n",
      "147 1\n",
      "148 0\n",
      "149 0\n",
      "150 8\n",
      "151 3\n",
      "152 2\n",
      "153 1\n",
      "154 9\n",
      "155 0\n",
      "156 0\n",
      "157 1\n",
      "158 0\n",
      "159 7\n",
      "160 0\n",
      "161 0\n",
      "162 4\n",
      "163 0\n",
      "164 4\n",
      "165 0\n",
      "166 0\n",
      "167 7\n",
      "168 0\n",
      "169 0\n",
      "170 3\n",
      "171 1\n",
      "172 0\n",
      "173 0\n",
      "174 4\n",
      "175 2\n",
      "176 0\n",
      "177 1\n",
      "178 2\n",
      "179 2\n",
      "180 2\n",
      "181 0\n",
      "182 0\n",
      "183 2\n",
      "184 2\n",
      "185 1\n",
      "186 1\n",
      "187 6\n",
      "188 0\n",
      "189 0\n",
      "190 0\n",
      "191 4\n",
      "192 1\n",
      "193 0\n",
      "194 0\n",
      "195 0\n",
      "196 0\n",
      "197 0\n",
      "198 0\n",
      "199 0\n",
      "200 1\n",
      "201 1\n",
      "202 1\n",
      "203 0\n",
      "204 0\n",
      "205 4\n",
      "206 5\n",
      "207 1\n",
      "208 2\n",
      "209 0\n",
      "210 0\n",
      "211 1\n",
      "212 5\n",
      "213 0\n",
      "214 4\n",
      "215 6\n",
      "216 2\n",
      "217 3\n",
      "218 2\n",
      "219 1\n",
      "220 0\n",
      "221 2\n",
      "222 0\n",
      "223 0\n",
      "224 1\n",
      "225 0\n",
      "226 3\n",
      "227 1\n",
      "228 2\n",
      "229 2\n",
      "230 3\n",
      "231 0\n",
      "232 0\n",
      "233 1\n",
      "234 2\n",
      "235 0\n",
      "236 0\n",
      "237 0\n",
      "238 3\n",
      "239 4\n",
      "240 0\n",
      "241 0\n",
      "242 0\n",
      "243 0\n",
      "244 0\n",
      "245 3\n",
      "246 0\n",
      "247 0\n",
      "248 0\n",
      "249 3\n",
      "250 3\n",
      "251 3\n",
      "252 0\n",
      "253 0\n",
      "254 1\n",
      "255 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../parsed_CSVs/stage_2/treatment256.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/cv2gnnv97pj8pmspmsfx96v00000gn/T/ipykernel_47132/100825137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m301\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_file_path_of_parsed_csvs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mriv_fraction_lost_col_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RIV_fractionLost_ellen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mriv_fraction_lost_col_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RIV_fractionLost_aadya'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../parsed_CSVs/stage_2/treatment256.csv'"
     ]
    }
   ],
   "source": [
    "parent_file_path_of_parsed_csvs = \"../parsed_CSVs/stage_2/treatment\"\n",
    "\n",
    "for i in range(1, 301):\n",
    "\n",
    "    df = pd.read_csv(parent_file_path_of_parsed_csvs + str(i) + \".csv\")\n",
    "    riv_fraction_lost_col_e = df['RIV_fractionLost_ellen']\n",
    "    riv_fraction_lost_col_a = df['RIV_fractionLost_aadya']\n",
    "    tally = 0\n",
    "    for element in riv_fraction_lost_col_e:\n",
    "        if element > 0:\n",
    "            tally +=1 \n",
    "    for element in riv_fraction_lost_col_a:\n",
    "        if element > 0:\n",
    "            tally +=1\n",
    "    \n",
    "    print(i, tally)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"independent_vars_tables/stage_1_independent_vars_table.csv\")\n",
    "df2 = pd.read_csv(\"independent_vars_tables/stage_1_independent_vars_table222.csv\")\n",
    "\n",
    "are_equal = df1.equals(df2)\n",
    "print(are_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
