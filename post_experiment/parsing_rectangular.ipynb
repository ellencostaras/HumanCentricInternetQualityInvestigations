{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data conversion functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE CONVERSION/CLEANING FUNCTIONS\n",
    "\n",
    "def iso_to_unix_time(iso_string):\n",
    "    '''funtion converting ISO time (like in Web RTC) to unix time'''\n",
    "\n",
    "    dt = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    unix_time = int(dt.timestamp())\n",
    "    return unix_time\n",
    "\n",
    "def convert_to_sec_minus_10_hrs(timestamp):\n",
    "    '''so for some reason Web RTC's timestamps are 10 hours later than the real time\n",
    "    of the call (24-hour time conversion glitch?). so this function converts a millisecond\n",
    "    unix timestamp into seconds, and takes away 10 hours.'''\n",
    "    new_timestamp = math.floor(float(timestamp) / 1000) - 36000\n",
    "    return new_timestamp\n",
    "\n",
    "def separate_by_comma(text_list):\n",
    "    '''Function which takes a list in text form and converts it to a proper Python list'''\n",
    "    \n",
    "    try:\n",
    "        # Use the `ast.literal_eval` method which safely evaluates a string containing\n",
    "        # a Python literal expression (e.g., a list).\n",
    "        parsed_list = ast.literal_eval(text_list)\n",
    "        \n",
    "        # Ensure the parsed output is a list\n",
    "        if isinstance(parsed_list, list):\n",
    "            return parsed_list\n",
    "        else:\n",
    "            raise ValueError(\"Input is not a list\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"Input is not properly formatted or is not a list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(file_path, verbose=False):\n",
    "    '''\n",
    "    This is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats \n",
    "    \n",
    "    This function parses the relevant stats and saves them in custom data types (dictionaries).\n",
    "    '''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    #navigate to where all the stats are stored in the dump\n",
    "    peer_connections = dump.get('PeerConnections', {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get('stats', {})\n",
    "    \n",
    "    #target substrings to pattern match for in stats\n",
    "    target_substrings_IT01V = [\n",
    "        '-[packetsReceived/s]',\n",
    "        '-packetsLost', \n",
    "        '-frameWidth', \n",
    "        '-framesPerSecond', \n",
    "        '-totalFreezesDuration',\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-totalProcessingDelay',\n",
    "        '-timestamp']\n",
    "    target_substrings_IT01A = [\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-timestamp']\n",
    "    target_substrings_OT01V = [\n",
    "        '-[packetsSent/s]',\n",
    "        '-[bytesSent_in_bits/s]',\n",
    "        '-frameWidth',\n",
    "        '-framesPerSecond',\n",
    "        '-totalPacketSendDelay',\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]',\n",
    "        '-qualityLimitationReason',\n",
    "        '-qualityLimitationResolutionChanges',\n",
    "        '-timestamp']\n",
    "    target_substrings_RIV = [\n",
    "        '-roundTripTime',\n",
    "        '-fractionLost',\n",
    "        '-timestamp']\n",
    "    target_substrings_RIA = [\n",
    "        '-fractionLost',\n",
    "        '-timestamp']\n",
    "    target_substrings_ROA = [\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_SV2 = [\n",
    "        '-width',\n",
    "        '-framesPerSecond',\n",
    "        '-timestamp']\n",
    "    target_substrings_AP = [\n",
    "        '-totalPlayoutDelay',\n",
    "        '-timestamp']\n",
    "    \n",
    "    #final dictionary data types to store all the values. \n",
    "    #each (None None None) triple will be filled with (values, start time, end time)\n",
    "    target_values_dict_IT01V = {\n",
    "        '-[packetsReceived/s]': (None, None, None),\n",
    "        '-packetsLost': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-totalFreezesDuration': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-totalProcessingDelay': (None, None, None),\n",
    "        '-jitter': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_IT01A = {\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_OT01V = {\n",
    "        '-[packetsSent/s]': (None, None, None),\n",
    "        '-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-totalPacketSendDelay': (None, None, None),\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]': (None, None, None),\n",
    "        '-qualityLimitationReason': (None, None, None),\n",
    "        '-qualityLimitationResolutionChanges': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIV = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIA = {\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_ROA = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_SV2 = {\n",
    "        '-width': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_AP = {\n",
    "        '-totalPlayoutDelay': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    \n",
    "    #begin searching for the target statistics\n",
    "    for key, value in stats.items():\n",
    "        key_string = str(key)\n",
    "        \n",
    "        # inbound video ones\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            for target_substring in target_substrings_IT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {}) #jump into the innermost dictionary\n",
    "                    if target_values_dict_IT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_IT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime'])) #just record whats in the values\n",
    "            #special case for finding jitter because it is a substring of other keys too\n",
    "            if key_string[-7:] == '-jitter':\n",
    "                info = stats.get(key, {})\n",
    "                if target_values_dict_IT01V['-jitter'] == (None, None, None):\n",
    "                    target_values_dict_IT01V['-jitter'] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # inbound audio ones\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            for target_substring in target_substrings_IT01A:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_IT01A[target_substring] == (None, None, None):\n",
    "                        target_values_dict_IT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "    \n",
    "        # outbound video ones\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            for target_substring in target_substrings_OT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_OT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_OT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # remote inbound video ones\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            for target_substring in target_substrings_RIV:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIV[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIV[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote inbound audio ones\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            for target_substring in target_substrings_RIA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote outbound audio ones\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            for target_substring in target_substrings_ROA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_ROA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_ROA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # video source ones\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            for target_substring in target_substrings_SV2:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_SV2[target_substring] == (None, None, None):\n",
    "                        target_values_dict_SV2[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # audio playout ones\n",
    "        elif key_string[:2] == 'AP':\n",
    "            for target_substring in target_substrings_AP:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_AP[target_substring] == (None, None, None):\n",
    "                        target_values_dict_AP[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "    # Making a global dictionary with unique keys names \n",
    "    single_person_dict = {\n",
    "        'IT01V_packetsRecieved': target_values_dict_IT01V['-[packetsReceived/s]'],\n",
    "        'IT01V_packetsLost': target_values_dict_IT01V['-packetsLost'],\n",
    "        'IT01V_frameWidth': target_values_dict_IT01V['-frameWidth'],\n",
    "        'IT01V_totalFreezesDuration': target_values_dict_IT01V['-totalFreezesDuration'],\n",
    "        'IT01V_framesPerSecond': target_values_dict_IT01V['-framesPerSecond'],\n",
    "        'IT01V_bytesReceived_in_bits/s': target_values_dict_IT01V['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01V_totalProcessingDelay': target_values_dict_IT01V['-totalProcessingDelay'],\n",
    "        'IT01V_jitter': target_values_dict_IT01V['-jitter'],\n",
    "        'IT01V_timestamps': target_values_dict_IT01V['-timestamp'],\n",
    "        'IT01A_bytesReceived_in_bits/s': target_values_dict_IT01A['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01A_timestamps': target_values_dict_IT01A['-timestamp'],\n",
    "        'OT01V_packetsSent/s': target_values_dict_OT01V['-[packetsSent/s]'],\n",
    "        'OT01V_bytesSent_in_bits/s': target_values_dict_OT01V['-[bytesSent_in_bits/s]'],\n",
    "        'OT01V_frameWidth': target_values_dict_OT01V['-frameWidth'],\n",
    "        'OT01V_framesPerSecond': target_values_dict_OT01V['-framesPerSecond'],\n",
    "        'OT01V_totalPacketSendDelay': target_values_dict_OT01V['-totalPacketSendDelay'],\n",
    "        'OT01V_totalPacketSendDelay/packetsSent_in_ms': target_values_dict_OT01V['-[totalPacketSendDelay/packetsSent_in_ms]'],\n",
    "        'OT01V_qualityLimitationReason': target_values_dict_OT01V['-qualityLimitationReason'],\n",
    "        'OT01V_qualityLimitationResolutionChanges': target_values_dict_OT01V['-qualityLimitationResolutionChanges'],\n",
    "        'OT01V_timestamps': target_values_dict_OT01V['-timestamp'],\n",
    "        'RIV_roundTripTime': target_values_dict_RIV['-roundTripTime'],\n",
    "        'RIV_fractionLost': target_values_dict_RIV['-fractionLost'],\n",
    "        'RIV_timestamps': target_values_dict_RIV['-timestamp'],\n",
    "        'RIA_fractionLost': target_values_dict_RIA['-fractionLost'],\n",
    "        'RIA_timestamps': target_values_dict_RIA['-timestamp'],\n",
    "        'ROA_roundTripTime': target_values_dict_ROA['-roundTripTime'],\n",
    "        'ROA_timestamps': target_values_dict_ROA['-timestamp'],\n",
    "        'SV2_width': target_values_dict_SV2['-width'],\n",
    "        'SV2_framesPerSecond': target_values_dict_SV2['-framesPerSecond'],\n",
    "        'SV2_timestamps': target_values_dict_SV2['-timestamp'],\n",
    "        'AP_totalPlayoutDelay': target_values_dict_AP['-totalPlayoutDelay'],\n",
    "        'AP_timestamps': target_values_dict_AP['-timestamp']}\n",
    "    \n",
    "    if verbose:    \n",
    "        for key, value in single_person_dict.items():\n",
    "            print(key, \": \", value[0])\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return single_person_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and formatting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dictionaries(dict_ellen, dict_aadya):\n",
    "    global_dict = {}\n",
    "    for key, val in dict_ellen.items():\n",
    "        key_string = str(key)\n",
    "        global_dict[key_string + \"_ellen\"] = val\n",
    "        global_dict[key_string + \"_aadya\"] = dict_aadya[key_string]\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "def populate_global_table(global_dict, mistake_tally):\n",
    "\n",
    "    global_start = 999999999999999999999999\n",
    "    global_end = 0\n",
    "    \n",
    "    for key, val in global_dict.items():\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        if start_time < global_start:\n",
    "            global_start = start_time\n",
    "        if end_time > global_end:\n",
    "            global_end = end_time\n",
    "\n",
    "    total_time = global_end - global_start + 1\n",
    "\n",
    "    #populate a rectangular table with -1 for every timestamp\n",
    "    global_table = []\n",
    "    for key, val in global_dict.items():\n",
    "        global_table.append([-1] * total_time)\n",
    "\n",
    "    #truncate timestamps to basic unix timecodes, (round to closest second)\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        if \"timestamps\" in key_string:\n",
    "            old_timestamps = val[0]\n",
    "            new_timestamps = []\n",
    "            for time in old_timestamps:\n",
    "                new_timestamps.append(convert_to_sec_minus_10_hrs(time))\n",
    "            global_dict[key] = (new_timestamps, val[1], val[2])\n",
    "\n",
    "    #replace -1s in the timestamps where data exists for every stat for ellen\n",
    "    row_number = 0\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        person = key_string[-5:]\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            timestamps = global_dict['IT01V_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            timestamps = global_dict['IT01A_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            timestamps = global_dict['OT01V_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            timestamps = global_dict['RIV_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            timestamps = global_dict['RIA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            timestamps = global_dict['ROA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            timestamps = global_dict['SV2_timestamps_' + person][0]\n",
    "        elif key_string[:2] == 'AP':\n",
    "            timestamps = global_dict['AP_timestamps_' + person][0]\n",
    "\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        \n",
    "        # Timing error handling:\n",
    "        if start_time < timestamps[0]:\n",
    "            start_time_index = 0 \n",
    "            mistake_tally['start time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        if end_time > timestamps[-1]:\n",
    "            end_time_index = 0\n",
    "            mistake_tally['end time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        \n",
    "        for time in range(len(timestamps)):\n",
    "            if timestamps[time] == start_time:\n",
    "                start_time_index = time\n",
    "            if timestamps[time] == end_time:\n",
    "                end_time_index = time \n",
    "        appropriate_timestamps = timestamps[start_time_index : end_time_index + 1]\n",
    "        \n",
    "        #sneaky cleaning in the cases where Web RTC makes a mistake:\n",
    "        if start_time < timestamps[0]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][difference:], val[1], val[2])\n",
    "        if end_time > timestamps[-1]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][:-difference], val[1], val[2])\n",
    "        \n",
    "        if len(appropriate_timestamps) != len(val[0]):\n",
    "            #print(\"Timing Error found:\", key, \"| len_times:\", len(appropriate_timestamps), \"| len_vals:\", len(val[0]))\n",
    "            difference = len(appropriate_timestamps) - len(val[0])\n",
    "            if difference == 1:\n",
    "                mistake_tally['missed val errors (off by 1 only)'] += 1\n",
    "            elif difference > 1:\n",
    "                mistake_tally['missed val errors (off by > 1)'].append(difference)\n",
    "                print(\"Timing Error found:\", key, \"| len_times:\", len(appropriate_timestamps), \"| len_vals:\", len(val[0]))\n",
    "            elif difference == -1:\n",
    "                mistake_tally['extra vals errors (off by 1 only)'] += 1\n",
    "            elif difference < -1:\n",
    "                mistake_tally['extra vals errors (off by > 1)'].append(-1 * difference)\n",
    "            appropriate_timestamps = appropriate_timestamps[ : -1 * abs(difference)] #bad but neccessary assumption LIMITATION LIMITATION LIMITATION\n",
    "\n",
    "        \n",
    "        for t in range(len(appropriate_timestamps)):\n",
    "            time = appropriate_timestamps[t]\n",
    "            global_table[row_number][time - global_start] = val[0][t]\n",
    "        row_number += 1\n",
    "    \n",
    "    return global_table\n",
    "\n",
    "\n",
    "def writeout(global_table, global_dict, treatment_number):\n",
    "\n",
    "    # Flip the table (rows -> columns and columns -> rows) for writeout nice-ness\n",
    "    global_table_flipped = []\n",
    "    for col in range(len(global_table[0])):\n",
    "        row_flipped = []\n",
    "        for row in range(len(global_table)):\n",
    "            row_flipped.append(global_table[row][col])\n",
    "        global_table_flipped.append(row_flipped)\n",
    "\n",
    "    # Write out to a CSV\n",
    "    output_file = f\"CSVs/stage_one/treatment{treatment_number}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        header = []\n",
    "        for key, val in global_dict.items():\n",
    "            header.append(key)\n",
    "        writer.writerow(header)\n",
    "        for row in global_table_flipped:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_writeout(parent_file_path, treatment_number, mistake_tally, read_only=False, verbose=False):\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "    dict_ellen = get_stats(file_path_ellen)\n",
    "    dict_aadya = get_stats(file_path_aadya)\n",
    "    global_dict = combine_dictionaries(dict_ellen, dict_aadya)\n",
    "    global_table = populate_global_table(global_dict, mistake_tally)\n",
    "    if not read_only:\n",
    "        writeout(global_table, global_dict, treatment_number)\n",
    "\n",
    "def parse_range(parent_file_path, lowest_treatment, highest_treatment, read_only=False, verbose=False):\n",
    "    mistake_tally = {\n",
    "        'start time errors': 0,\n",
    "        'end time errors': 0,\n",
    "        'missed val errors (off by 1 only)': 0,\n",
    "        'missed val errors (off by > 1)': [],\n",
    "        'extra vals errors (off by 1 only)': 0,\n",
    "        'extra vals errors (off by > 1)': []     \n",
    "    }\n",
    "\n",
    "    for i in range(lowest_treatment, highest_treatment + 1):\n",
    "        print(\"Parsing Treatment\", i)\n",
    "        parse_clean_writeout(parent_file_path, i, mistake_tally, read_only, verbose)\n",
    "\n",
    "    print(\"\\nParsing complete! Here is the number Web RTC timing errors encountered...\")\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    for key, val in mistake_tally.items():\n",
    "        print(key + \":\", val)\n",
    "    print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time start: 1725357518\n",
      "time end: 1725357630\n",
      "time stamps: 28\n",
      "fps start: 1725357520\n",
      "fps end: 1725357630\n",
      "fps vals: 22\n",
      "\n",
      "\n",
      "[1725357518, 1725357519, 1725357520, 1725357521, 1725357522, 1725357523, 1725357524, 1725357525, 1725357526, 1725357527, 1725357528, 1725357529, 1725357530, 1725357531, 1725357532, 1725357533, 1725357534, 1725357535, 1725357536, 1725357537, 1725357538, 1725357539, 1725357540, 1725357541, 1725357542, 1725357543, 1725357570, 1725357630]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(\"time start:\", iso_to_unix_time(\"2024-09-03T19:58:38.197Z\"))\n",
    "print(\"time end:\", iso_to_unix_time(\"2024-09-03T20:00:30.198Z\"))\n",
    "stamps = [1725393518197.914,1725393519198.508,1725393520197.92,1725393521197.562,1725393522197.712,1725393523198.336,1725393524198.55,1725393525199.036,1725393526198.197,1725393527197.728,1725393528197.608,1725393529198.402,1725393530198.551,1725393531198.004,1725393532198.027,1725393533198.221,1725393534198.171,1725393535197.804,1725393536197.74,1725393537197.821,1725393538197.857,1725393539198.626,1725393540197.957,1725393541198.044,1725393542197.778,1725393543198.655,1725393570198.228,1725393630198.615]\n",
    "print(\"time stamps:\", len(stamps))\n",
    "print(\"fps start:\", iso_to_unix_time(\"2024-09-03T19:58:40.197Z\"))\n",
    "print(\"fps end:\", iso_to_unix_time(\"2024-09-03T20:00:30.198Z\"))\n",
    "print(\"fps vals:\", len([4,3,34,31,30,31,30,28,30,30,30,30,30,30,28,30,31,30,30,30,29,30]))\n",
    "print(\"\\n\")\n",
    "new = []\n",
    "for stamp in stamps:\n",
    "    new.append(convert_to_sec_minus_10_hrs(stamp))\n",
    "print(new)\n",
    "for dud in range(len(new)):\n",
    "    dude = new[dud]\n",
    "    if dude == iso_to_unix_time(\"2024-09-03T10:05:40.068Z\"):\n",
    "        indi1 = dud\n",
    "        print(\"yep\")\n",
    "    elif dude == iso_to_unix_time(\"2024-09-03T10:07:25.067Z\"):\n",
    "        print(\"yep\")\n",
    "        indi2 = dud\n",
    "print(len(stamps[indi1:indi2+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Treatment 102\n",
      "Timing Error found: IT01V_framesPerSecond_aadya | len_times: 26 | len_vals: 22\n",
      "Data has been written to CSVs/stage_one/treatment102.csv\n",
      "\n",
      "Parsing complete! Here is the number Web RTC timing errors encountered...\n",
      "___________________________________________________________________________\n",
      "start time errors: 0\n",
      "end time errors: 0\n",
      "missed val errors (off by 1 only): 1\n",
      "missed val errors (off by > 1): [4]\n",
      "extra vals errors (off by 1 only): 0\n",
      "extra vals errors (off by > 1): []\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#file paths for testing_9_Aug\n",
    "file_path_parent_01 = \"testing_stats/testing_9_Aug/treatment\"\n",
    "#file path parent for testing_13_Aug\n",
    "file_path_parent_02 = \"testing_stats/testing_13_Aug/treatment\" \n",
    "#file path parent for testing_27_Aug\n",
    "file_path_parent_03 = \"testing_stats/testing_27_Aug/treatment\"\n",
    "#file path parent for testing_30_Aug\n",
    "file_path_parent_04 = \"testing_stats/testing_30_Aug/treatment\"\n",
    "#file path parent for stage one treatments\n",
    "file_path_parent_05 = \"testing_stats/stage_1/treatment\"\n",
    "\n",
    "lowest_treatment_number = 102\n",
    "highest_treatment_number = 102\n",
    "read_only_status = False\n",
    "verbose_status = False\n",
    "parse_range(file_path_parent_05, lowest_treatment_number, highest_treatment_number, read_only_status, verbose_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
