{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import ast\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data conversion functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE CONVERSION/CLEANING FUNCTIONS\n",
    "\n",
    "def iso_to_unix_time(iso_string):\n",
    "    '''funtion converting ISO time (like in Web RTC) to unix time'''\n",
    "\n",
    "    dt = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    unix_time = int(dt.timestamp())\n",
    "    return unix_time\n",
    "\n",
    "def convert_to_sec_minus_10_hrs(timestamp):\n",
    "    '''so for some reason Web RTC's timestamps are 10 hours later than the real time\n",
    "    of the call (24-hour time conversion glitch?). so this function converts a millisecond\n",
    "    unix timestamp into seconds, and takes away 10 hours.'''\n",
    "    new_timestamp = math.floor(float(timestamp) / 1000) - 36000\n",
    "    return new_timestamp\n",
    "\n",
    "def separate_by_comma(text_list):\n",
    "    '''Function which takes a list in text form and converts it to a proper Python list'''\n",
    "    \n",
    "    try:\n",
    "        # Use the `ast.literal_eval` method which safely evaluates a string containing\n",
    "        # a Python literal expression (e.g., a list).\n",
    "        parsed_list = ast.literal_eval(text_list)\n",
    "        \n",
    "        # Ensure the parsed output is a list\n",
    "        if isinstance(parsed_list, list):\n",
    "            return parsed_list\n",
    "        else:\n",
    "            raise ValueError(\"Input is not a list\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"Input is not properly formatted or is not a list\")\n",
    "    \n",
    "def parse_audio_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_config = string[char+1:]\n",
    "                break\n",
    "        if \"usedtx\" in codec_config:\n",
    "            dtx = True\n",
    "        else:\n",
    "            dtx = False\n",
    "        if \"useinbandfec\" in codec_config:\n",
    "            fec = True\n",
    "        else:\n",
    "            fec = False\n",
    "        new_codec_info.append((dtx, fec))\n",
    "    \n",
    "    return new_codec_info\n",
    "\n",
    "def parse_video_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_name = string[:char]\n",
    "                break\n",
    "        new_codec_info.append(codec_name)\n",
    "\n",
    "    return new_codec_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(file_path, verbose=False):\n",
    "    '''\n",
    "    This is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats \n",
    "    \n",
    "    This function parses the relevant stats and saves them in custom data types (dictionaries).\n",
    "    '''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    #navigate to where all the stats are stored in the dump\n",
    "    peer_connections = dump.get('PeerConnections', {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get('stats', {})\n",
    "    \n",
    "    #target substrings to pattern match for in stats\n",
    "    target_substrings_IT01V = [\n",
    "        '-[packetsReceived/s]',\n",
    "        '-packetsLost', \n",
    "        '-frameWidth', \n",
    "        '-frameHeight',\n",
    "        '-framesPerSecond', \n",
    "        '-totalFreezesDuration',\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-totalProcessingDelay',\n",
    "        '-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_IT01A = [\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_OT01V = [\n",
    "        '-[packetsSent/s]',\n",
    "        '-[bytesSent_in_bits/s]',\n",
    "        '-frameWidth',\n",
    "        '-framesPerSecond',\n",
    "        '-totalPacketSendDelay',\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]',\n",
    "        '-qualityLimitationReason',\n",
    "        '-qualityLimitationResolutionChanges',\n",
    "        '-timestamp']\n",
    "    #target_substrings_OT01A = [\n",
    "        #'-[bytesSent_in_bits/s]',\n",
    "        #'-timestamp']\n",
    "    target_substrings_RIV = [\n",
    "        '-roundTripTime',\n",
    "        '-fractionLost',\n",
    "        '-timestamp']\n",
    "    target_substrings_RIA = [\n",
    "        '-fractionLost',\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_ROA = [\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_SV2 = [\n",
    "        '-width',\n",
    "        '-height',\n",
    "        '-framesPerSecond',\n",
    "        '-timestamp']\n",
    "    target_substrings_AP = [\n",
    "        '-totalPlayoutDelay',\n",
    "        '-timestamp']\n",
    "    \n",
    "    #final dictionary data types to store all the values. \n",
    "    #each (None None None) triple will be filled with (values, start time, end time)\n",
    "    target_values_dict_IT01V = {\n",
    "        '-[packetsReceived/s]': (None, None, None),\n",
    "        '-packetsLost': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-frameHeight': (None, None, None),\n",
    "        '-totalFreezesDuration': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-totalProcessingDelay': (None, None, None),\n",
    "        '-jitter': (None, None, None),\n",
    "        '-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_IT01A = {\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_OT01V = {\n",
    "        '-[packetsSent/s]': (None, None, None),\n",
    "        '-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-totalPacketSendDelay': (None, None, None),\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]': (None, None, None),\n",
    "        '-qualityLimitationReason': (None, None, None),\n",
    "        '-qualityLimitationResolutionChanges': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    #target_values_dict_OT01A = {\n",
    "        #'-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        #'-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIV = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIA = {\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_ROA = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_SV2 = {\n",
    "        '-width': (None, None, None),\n",
    "        '-height': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_AP = {\n",
    "        '-totalPlayoutDelay': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    \n",
    "    #begin searching for the target statistics\n",
    "    for key, value in stats.items():\n",
    "        key_string = str(key)\n",
    "        \n",
    "        # inbound video ones\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            for target_substring in target_substrings_IT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {}) #jump into the innermost dictionary\n",
    "                    if target_values_dict_IT01V[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01V[target_substring] = (parse_video_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "            #special case for finding jitter because it is a substring of other keys too\n",
    "            if key_string[-7:] == '-jitter':\n",
    "                info = stats.get(key, {})\n",
    "                if target_values_dict_IT01V['-jitter'] == (None, None, None):\n",
    "                    target_values_dict_IT01V['-jitter'] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # inbound audio ones\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            for target_substring in target_substrings_IT01A:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_IT01A[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01A[target_substring] = (parse_audio_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "    \n",
    "        # outbound video ones\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            for target_substring in target_substrings_OT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_OT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_OT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # outbound audio ones\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #for target_substring in target_substrings_OT01A:\n",
    "                #if target_substring in key_string:\n",
    "                    #info = stats.get(key, {})\n",
    "                    #if target_values_dict_OT01A[target_substring] == (None, None, None):\n",
    "                        #target_values_dict_OT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # remote inbound video ones\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            for target_substring in target_substrings_RIV:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIV[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIV[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote inbound audio ones\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            for target_substring in target_substrings_RIA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote outbound audio ones\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            for target_substring in target_substrings_ROA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_ROA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_ROA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # video source ones\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            for target_substring in target_substrings_SV2:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_SV2[target_substring] == (None, None, None):\n",
    "                        target_values_dict_SV2[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # audio playout ones\n",
    "        elif key_string[:2] == 'AP':\n",
    "            for target_substring in target_substrings_AP:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_AP[target_substring] == (None, None, None):\n",
    "                        target_values_dict_AP[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "    # Making a global dictionary with unique keys names \n",
    "    single_person_dict = {\n",
    "        'IT01V_packetsRecieved': target_values_dict_IT01V['-[packetsReceived/s]'],\n",
    "        'IT01V_packetsLost': target_values_dict_IT01V['-packetsLost'],\n",
    "        'IT01V_frameWidth': target_values_dict_IT01V['-frameWidth'],\n",
    "        'IT01V_frameHeight': target_values_dict_IT01V['-frameHeight'],\n",
    "        'IT01V_totalFreezesDuration': target_values_dict_IT01V['-totalFreezesDuration'],\n",
    "        'IT01V_framesPerSecond': target_values_dict_IT01V['-framesPerSecond'],\n",
    "        'IT01V_bytesReceived_in_bits/s': target_values_dict_IT01V['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01V_totalProcessingDelay': target_values_dict_IT01V['-totalProcessingDelay'],\n",
    "        'IT01V_jitter': target_values_dict_IT01V['-jitter'],\n",
    "        'IT01V_jitterBufferDelay/emissions': target_values_dict_IT01V['-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]'],\n",
    "        'IT01V_codec': target_values_dict_IT01V['-[codec]'],\n",
    "        'IT01V_timestamps': target_values_dict_IT01V['-timestamp'],\n",
    "        'IT01A_bytesReceived_in_bits/s': target_values_dict_IT01A['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01A_jitterBufferDelay/emissions': target_values_dict_IT01A['-[jitterBufferDelay/jitterBufferEmittedCount_in_ms]'],\n",
    "        'IT01A_(dtx, fec)': target_values_dict_IT01A['-[codec]'],\n",
    "        'IT01A_timestamps': target_values_dict_IT01A['-timestamp'],\n",
    "        'OT01V_packetsSent/s': target_values_dict_OT01V['-[packetsSent/s]'],\n",
    "        'OT01V_bytesSent_in_bits/s': target_values_dict_OT01V['-[bytesSent_in_bits/s]'],\n",
    "        'OT01V_frameWidth': target_values_dict_OT01V['-frameWidth'],\n",
    "        'OT01V_framesPerSecond': target_values_dict_OT01V['-framesPerSecond'],\n",
    "        'OT01V_totalPacketSendDelay': target_values_dict_OT01V['-totalPacketSendDelay'],\n",
    "        'OT01V_totalPacketSendDelay/packetsSent_in_ms': target_values_dict_OT01V['-[totalPacketSendDelay/packetsSent_in_ms]'],\n",
    "        'OT01V_qualityLimitationReason': target_values_dict_OT01V['-qualityLimitationReason'],\n",
    "        'OT01V_qualityLimitationResolutionChanges': target_values_dict_OT01V['-qualityLimitationResolutionChanges'],\n",
    "        'OT01V_timestamps': target_values_dict_OT01V['-timestamp'],\n",
    "        'RIV_roundTripTime': target_values_dict_RIV['-roundTripTime'],\n",
    "        'RIV_fractionLost': target_values_dict_RIV['-fractionLost'],\n",
    "        'RIV_timestamps': target_values_dict_RIV['-timestamp'],\n",
    "        'RIA_fractionLost': target_values_dict_RIA['-fractionLost'],\n",
    "        'RIA_roundTripTime': target_values_dict_RIA['-roundTripTime'],\n",
    "        'RIA_timestamps': target_values_dict_RIA['-timestamp'],\n",
    "        'ROA_roundTripTime': target_values_dict_ROA['-roundTripTime'],\n",
    "        'ROA_timestamps': target_values_dict_ROA['-timestamp'],\n",
    "        'SV2_width': target_values_dict_SV2['-width'],\n",
    "        'SV2_height': target_values_dict_SV2['-height'],\n",
    "        'SV2_framesPerSecond': target_values_dict_SV2['-framesPerSecond'],\n",
    "        'SV2_timestamps': target_values_dict_SV2['-timestamp'],\n",
    "        'AP_totalPlayoutDelay': target_values_dict_AP['-totalPlayoutDelay'],\n",
    "        'AP_timestamps': target_values_dict_AP['-timestamp']}\n",
    "    \n",
    "    if verbose:    \n",
    "        for key, value in single_person_dict.items():\n",
    "            print(key, \": \", value[0])\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return single_person_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and formatting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dictionaries(dict_ellen, dict_aadya):\n",
    "    global_dict = {}\n",
    "    for key, val in dict_ellen.items():\n",
    "        key_string = str(key)\n",
    "        global_dict[key_string + \"_ellen\"] = val\n",
    "        global_dict[key_string + \"_aadya\"] = dict_aadya[key_string]\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "def populate_global_table(global_dict, mistake_tally):\n",
    "\n",
    "    global_start = 999999999999999999999999\n",
    "    global_end = 0\n",
    "    \n",
    "    for key, val in global_dict.items():\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        if start_time < global_start:\n",
    "            global_start = start_time\n",
    "        if end_time > global_end:\n",
    "            global_end = end_time\n",
    "\n",
    "    total_time = global_end - global_start + 1\n",
    "\n",
    "    #populate a rectangular table with -1 for every timestamp\n",
    "    global_table = []\n",
    "    for key, val in global_dict.items():\n",
    "        global_table.append([-1] * total_time)\n",
    "\n",
    "    #truncate timestamps to basic unix timecodes, (round to closest second)\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        if \"timestamps\" in key_string:\n",
    "            old_timestamps = val[0]\n",
    "            new_timestamps = []\n",
    "            for time in old_timestamps:\n",
    "                new_timestamps.append(convert_to_sec_minus_10_hrs(time))\n",
    "            global_dict[key] = (new_timestamps, val[1], val[2])\n",
    "\n",
    "    #replace -1s in the timestamps where data exists for every stat for ellen\n",
    "    row_number = 0\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        person = key_string[-5:]\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            timestamps = global_dict['IT01V_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            timestamps = global_dict['IT01A_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            timestamps = global_dict['OT01V_timestamps_' + person][0]\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #timestamps = global_dict['OT01A_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            timestamps = global_dict['RIV_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            timestamps = global_dict['RIA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            timestamps = global_dict['ROA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            timestamps = global_dict['SV2_timestamps_' + person][0]\n",
    "        elif key_string[:2] == 'AP':\n",
    "            timestamps = global_dict['AP_timestamps_' + person][0]\n",
    "\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        \n",
    "        # Timing error handling:\n",
    "        if start_time < timestamps[0]:\n",
    "            start_time_index = 0 \n",
    "            mistake_tally['start time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        if end_time > timestamps[-1]:\n",
    "            end_time_index = 0\n",
    "            mistake_tally['end time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        \n",
    "        for time in range(len(timestamps)):\n",
    "            if timestamps[time] == start_time:\n",
    "                start_time_index = time\n",
    "            if timestamps[time] == end_time:\n",
    "                end_time_index = time \n",
    "        appropriate_timestamps = timestamps[start_time_index : end_time_index + 1]\n",
    "        \n",
    "        #sneaky cleaning in the cases where Web RTC makes a mistake:\n",
    "        if start_time < timestamps[0]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][difference:], val[1], val[2])\n",
    "        if end_time > timestamps[-1]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][:-difference], val[1], val[2])\n",
    "        \n",
    "        if len(appropriate_timestamps) != len(val[0]):\n",
    "            #print(\"Timing Error found:\", key, \"| len_times:\", len(appropriate_timestamps), \"| len_vals:\", len(val[0]))\n",
    "            difference = len(appropriate_timestamps) - len(val[0])\n",
    "            if difference == 1:\n",
    "                mistake_tally['missed val errors (off by 1 only)'] += 1\n",
    "            elif difference > 1:\n",
    "                mistake_tally['missed val errors (off by > 1)'].append(difference)\n",
    "            elif difference == -1:\n",
    "                mistake_tally['extra vals errors (off by 1 only)'] += 1\n",
    "            elif difference < -1:\n",
    "                mistake_tally['extra vals errors (off by > 1)'].append(-1 * difference)\n",
    "            appropriate_timestamps = appropriate_timestamps[ : -1 * abs(difference)] #bad but neccessary assumption LIMITATION LIMITATION LIMITATION\n",
    "\n",
    "        \n",
    "        for t in range(len(appropriate_timestamps)):\n",
    "            time = appropriate_timestamps[t]\n",
    "            if isinstance(val[0][t], numbers.Number):\n",
    "                if val[0][t] >= -1:\n",
    "                    global_table[row_number][time - global_start] = val[0][t] \n",
    "                else:\n",
    "                    global_table[row_number][time - global_start] = 0\n",
    "            else:\n",
    "                global_table[row_number][time - global_start] = val[0][t]\n",
    "            \n",
    "            '''else: #handling for the rogue large negative erronous values Web RTC occasionally produces\n",
    "                if (t-1 > 0) and (t+1 < len(val[0]) - 1): #checking we aren't right on the edge of the list\n",
    "                    if (val[0][t-1] == -1) and (val[0][t+1] == -1): #if every second timestep is being recorded\n",
    "                        if (t-2 > 0) and (t-2 < len(val[0]) - 1):\n",
    "                            if (val[0][t-2] != -1) and (val[0][t+2] != -1): #situations like [..., x, -1, error, -1, y, ...]\n",
    "                                left = val[0][t-2]\n",
    "                                right = val[0][t+2]\n",
    "                                if isinstance(left, float) or isinstance(right, float):\n",
    "                                    val[0][t] = (left + right) / 2 #replace the error with the average of it's left-right neighbours\n",
    "                                elif isinstance(left, int):\n",
    "                                    val[0][t] = round((left + right) / 2) #replace the error with the rounded average of it's left-right neighbours\n",
    "                                else:\n",
    "                                    val[0][t] = left #if stat's data are all strings (e.g \"(True, False)\"), replace error with previous entry\n",
    "                    elif (val[0][t-1] != -1) and (val[0][t+1] != -1): #situations like [..., x, error, y, ...]\n",
    "                        left = val[0][t-1]\n",
    "                        right = val[0][t+1]\n",
    "                        if isinstance(left, float) or isinstance(right, float):\n",
    "                            val[0][t] = (left + right) / 2 #replace the error with the average of it's left-right neighbours\n",
    "                        elif isinstance(left, int):\n",
    "                            val[0][t] = round((left + right) / 2) #replace the error with the rounded average of it's left-right neighbours\n",
    "                        else:\n",
    "                            val[0][t] = left #if stat's data are all strings (e.g \"(True, False)\"), replace error with previous entry\n",
    "                    else:\n",
    "                        val[0][t] = -1 #in all other situations, replace with -1 \n",
    "                global_table[row_number][time - global_start] = val[0][t]'''\n",
    "        \n",
    "        row_number += 1\n",
    "    \n",
    "    return global_table\n",
    "\n",
    "\n",
    "def fix_unwanted_nulls(global_table):\n",
    "    '''function that iterates through a populated global table, finding instances where a stat \n",
    "    only has values recorded once every two seconds, and replaces the empty-second entries with\n",
    "    the average on the values on either side.'''\n",
    "\n",
    "    for stat in global_table:\n",
    "        for i in range(1, len(stat) - 1):\n",
    "            if (stat[i-1] != -1) and (stat[i] == -1) and (stat[i+1] != -1):\n",
    "                \n",
    "                if isinstance(stat[i-1], float) or isinstance(stat[i+1], float):\n",
    "                    stat[i] = (stat[i-1] + stat[i+1]) / 2 #replace the null with the average of it's left-right neighbours\n",
    "                \n",
    "                elif isinstance(stat[i-1], int):\n",
    "                    stat[i] = round((stat[i-1] + stat[i+1]) / 2) #replace the null with the rounded average of it's left-right neighbours\n",
    "                \n",
    "                else:\n",
    "                    stat[i] = stat[i-1] #if stat's data are all strings (e.g \"(True, False)\"), replace null with previous entry\n",
    "\n",
    "\n",
    "def writeout(global_table, global_dict, treatment_number):\n",
    "\n",
    "    # Flip the table (rows -> columns and columns -> rows) for writeout nice-ness\n",
    "    global_table_flipped = []\n",
    "    for col in range(len(global_table[0])):\n",
    "        row_flipped = []\n",
    "        for row in range(len(global_table)):\n",
    "            row_flipped.append(global_table[row][col])\n",
    "        global_table_flipped.append(row_flipped)\n",
    "\n",
    "    # Write out to a CSV\n",
    "    output_file = f\"CSVs/stage_two/treatment{treatment_number}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        header = []\n",
    "        for key, val in global_dict.items():\n",
    "            header.append(key)\n",
    "        writer.writerow(header)\n",
    "        for row in global_table_flipped:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_writeout(parent_file_path, treatment_number, mistake_tally, read_only=False, verbose=False):\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "    dict_ellen = get_stats(file_path_ellen)\n",
    "    dict_aadya = get_stats(file_path_aadya)\n",
    "    global_dict = combine_dictionaries(dict_ellen, dict_aadya)\n",
    "    global_table = populate_global_table(global_dict, mistake_tally)\n",
    "    fix_unwanted_nulls(global_table)\n",
    "    if not read_only:\n",
    "        writeout(global_table, global_dict, treatment_number)\n",
    "\n",
    "def parse_range(parent_file_path, lowest_treatment, highest_treatment, read_only=False, verbose=False):\n",
    "    mistake_tally = {\n",
    "        'start time errors': 0,\n",
    "        'end time errors': 0,\n",
    "        'missed val errors (off by 1 only)': 0,\n",
    "        'missed val errors (off by > 1)': [],\n",
    "        'extra vals errors (off by 1 only)': 0,\n",
    "        'extra vals errors (off by > 1)': []     \n",
    "    }\n",
    "\n",
    "    for i in range(lowest_treatment, highest_treatment + 1):\n",
    "        print(\"Parsing Treatment\", i)\n",
    "        parse_clean_writeout(parent_file_path, i, mistake_tally, read_only, verbose)\n",
    "\n",
    "    print(\"\\nParsing complete! Here is the number Web RTC timing errors encountered...\")\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    for key, val in mistake_tally.items():\n",
    "        print(key + \":\", val)\n",
    "    print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Treatment 1\n",
      "Data has been written to CSVs/stage_two/treatment1.csv\n",
      "Parsing Treatment 2\n",
      "Data has been written to CSVs/stage_two/treatment2.csv\n",
      "Parsing Treatment 3\n",
      "Data has been written to CSVs/stage_two/treatment3.csv\n",
      "Parsing Treatment 4\n",
      "Data has been written to CSVs/stage_two/treatment4.csv\n",
      "Parsing Treatment 5\n",
      "Data has been written to CSVs/stage_two/treatment5.csv\n",
      "Parsing Treatment 6\n",
      "Data has been written to CSVs/stage_two/treatment6.csv\n",
      "Parsing Treatment 7\n",
      "Data has been written to CSVs/stage_two/treatment7.csv\n",
      "Parsing Treatment 8\n",
      "Data has been written to CSVs/stage_two/treatment8.csv\n",
      "Parsing Treatment 9\n",
      "Data has been written to CSVs/stage_two/treatment9.csv\n",
      "Parsing Treatment 10\n",
      "Data has been written to CSVs/stage_two/treatment10.csv\n",
      "Parsing Treatment 11\n",
      "Data has been written to CSVs/stage_two/treatment11.csv\n",
      "Parsing Treatment 12\n",
      "Data has been written to CSVs/stage_two/treatment12.csv\n",
      "Parsing Treatment 13\n",
      "Data has been written to CSVs/stage_two/treatment13.csv\n",
      "Parsing Treatment 14\n",
      "Data has been written to CSVs/stage_two/treatment14.csv\n",
      "Parsing Treatment 15\n",
      "Data has been written to CSVs/stage_two/treatment15.csv\n",
      "Parsing Treatment 16\n",
      "Data has been written to CSVs/stage_two/treatment16.csv\n",
      "Parsing Treatment 17\n",
      "Data has been written to CSVs/stage_two/treatment17.csv\n",
      "Parsing Treatment 18\n",
      "Data has been written to CSVs/stage_two/treatment18.csv\n",
      "Parsing Treatment 19\n",
      "Data has been written to CSVs/stage_two/treatment19.csv\n",
      "Parsing Treatment 20\n",
      "Data has been written to CSVs/stage_two/treatment20.csv\n",
      "Parsing Treatment 21\n",
      "Data has been written to CSVs/stage_two/treatment21.csv\n",
      "Parsing Treatment 22\n",
      "Data has been written to CSVs/stage_two/treatment22.csv\n",
      "Parsing Treatment 23\n",
      "Data has been written to CSVs/stage_two/treatment23.csv\n",
      "Parsing Treatment 24\n",
      "Data has been written to CSVs/stage_two/treatment24.csv\n",
      "Parsing Treatment 25\n",
      "Data has been written to CSVs/stage_two/treatment25.csv\n",
      "Parsing Treatment 26\n",
      "Data has been written to CSVs/stage_two/treatment26.csv\n",
      "Parsing Treatment 27\n",
      "Data has been written to CSVs/stage_two/treatment27.csv\n",
      "Parsing Treatment 28\n",
      "Data has been written to CSVs/stage_two/treatment28.csv\n",
      "Parsing Treatment 29\n",
      "Data has been written to CSVs/stage_two/treatment29.csv\n",
      "Parsing Treatment 30\n",
      "Data has been written to CSVs/stage_two/treatment30.csv\n",
      "Parsing Treatment 31\n",
      "Data has been written to CSVs/stage_two/treatment31.csv\n",
      "Parsing Treatment 32\n",
      "Data has been written to CSVs/stage_two/treatment32.csv\n",
      "Parsing Treatment 33\n",
      "Data has been written to CSVs/stage_two/treatment33.csv\n",
      "Parsing Treatment 34\n",
      "Data has been written to CSVs/stage_two/treatment34.csv\n",
      "Parsing Treatment 35\n",
      "Data has been written to CSVs/stage_two/treatment35.csv\n",
      "Parsing Treatment 36\n",
      "Data has been written to CSVs/stage_two/treatment36.csv\n",
      "Parsing Treatment 37\n",
      "Data has been written to CSVs/stage_two/treatment37.csv\n",
      "Parsing Treatment 38\n",
      "Data has been written to CSVs/stage_two/treatment38.csv\n",
      "Parsing Treatment 39\n",
      "Data has been written to CSVs/stage_two/treatment39.csv\n",
      "Parsing Treatment 40\n",
      "Data has been written to CSVs/stage_two/treatment40.csv\n",
      "Parsing Treatment 41\n",
      "Data has been written to CSVs/stage_two/treatment41.csv\n",
      "Parsing Treatment 42\n",
      "Data has been written to CSVs/stage_two/treatment42.csv\n",
      "Parsing Treatment 43\n",
      "Data has been written to CSVs/stage_two/treatment43.csv\n",
      "Parsing Treatment 44\n",
      "Data has been written to CSVs/stage_two/treatment44.csv\n",
      "Parsing Treatment 45\n",
      "Data has been written to CSVs/stage_two/treatment45.csv\n",
      "Parsing Treatment 46\n",
      "Data has been written to CSVs/stage_two/treatment46.csv\n",
      "Parsing Treatment 47\n",
      "Data has been written to CSVs/stage_two/treatment47.csv\n",
      "Parsing Treatment 48\n",
      "Data has been written to CSVs/stage_two/treatment48.csv\n",
      "Parsing Treatment 49\n",
      "Data has been written to CSVs/stage_two/treatment49.csv\n",
      "Parsing Treatment 50\n",
      "Data has been written to CSVs/stage_two/treatment50.csv\n",
      "Parsing Treatment 51\n",
      "Data has been written to CSVs/stage_two/treatment51.csv\n",
      "Parsing Treatment 52\n",
      "Data has been written to CSVs/stage_two/treatment52.csv\n",
      "Parsing Treatment 53\n",
      "Data has been written to CSVs/stage_two/treatment53.csv\n",
      "Parsing Treatment 54\n",
      "Data has been written to CSVs/stage_two/treatment54.csv\n",
      "Parsing Treatment 55\n",
      "Data has been written to CSVs/stage_two/treatment55.csv\n",
      "Parsing Treatment 56\n",
      "Data has been written to CSVs/stage_two/treatment56.csv\n",
      "Parsing Treatment 57\n",
      "Data has been written to CSVs/stage_two/treatment57.csv\n",
      "Parsing Treatment 58\n",
      "Data has been written to CSVs/stage_two/treatment58.csv\n",
      "Parsing Treatment 59\n",
      "Data has been written to CSVs/stage_two/treatment59.csv\n",
      "Parsing Treatment 60\n",
      "Data has been written to CSVs/stage_two/treatment60.csv\n",
      "Parsing Treatment 61\n",
      "Data has been written to CSVs/stage_two/treatment61.csv\n",
      "Parsing Treatment 62\n",
      "Data has been written to CSVs/stage_two/treatment62.csv\n",
      "Parsing Treatment 63\n",
      "Data has been written to CSVs/stage_two/treatment63.csv\n",
      "Parsing Treatment 64\n",
      "Data has been written to CSVs/stage_two/treatment64.csv\n",
      "Parsing Treatment 65\n",
      "Data has been written to CSVs/stage_two/treatment65.csv\n",
      "Parsing Treatment 66\n",
      "Data has been written to CSVs/stage_two/treatment66.csv\n",
      "Parsing Treatment 67\n",
      "Data has been written to CSVs/stage_two/treatment67.csv\n",
      "Parsing Treatment 68\n",
      "Data has been written to CSVs/stage_two/treatment68.csv\n",
      "Parsing Treatment 69\n",
      "Data has been written to CSVs/stage_two/treatment69.csv\n",
      "Parsing Treatment 70\n",
      "Data has been written to CSVs/stage_two/treatment70.csv\n",
      "Parsing Treatment 71\n",
      "Data has been written to CSVs/stage_two/treatment71.csv\n",
      "Parsing Treatment 72\n",
      "Data has been written to CSVs/stage_two/treatment72.csv\n",
      "Parsing Treatment 73\n",
      "Data has been written to CSVs/stage_two/treatment73.csv\n",
      "Parsing Treatment 74\n",
      "Data has been written to CSVs/stage_two/treatment74.csv\n",
      "Parsing Treatment 75\n",
      "Data has been written to CSVs/stage_two/treatment75.csv\n",
      "Parsing Treatment 76\n",
      "Data has been written to CSVs/stage_two/treatment76.csv\n",
      "Parsing Treatment 77\n",
      "Data has been written to CSVs/stage_two/treatment77.csv\n",
      "Parsing Treatment 78\n",
      "Data has been written to CSVs/stage_two/treatment78.csv\n",
      "Parsing Treatment 79\n",
      "Data has been written to CSVs/stage_two/treatment79.csv\n",
      "Parsing Treatment 80\n",
      "Data has been written to CSVs/stage_two/treatment80.csv\n",
      "Parsing Treatment 81\n",
      "Data has been written to CSVs/stage_two/treatment81.csv\n",
      "Parsing Treatment 82\n",
      "Data has been written to CSVs/stage_two/treatment82.csv\n",
      "Parsing Treatment 83\n",
      "Data has been written to CSVs/stage_two/treatment83.csv\n",
      "Parsing Treatment 84\n",
      "Data has been written to CSVs/stage_two/treatment84.csv\n",
      "Parsing Treatment 85\n",
      "Data has been written to CSVs/stage_two/treatment85.csv\n",
      "Parsing Treatment 86\n",
      "Data has been written to CSVs/stage_two/treatment86.csv\n",
      "Parsing Treatment 87\n",
      "Data has been written to CSVs/stage_two/treatment87.csv\n",
      "Parsing Treatment 88\n",
      "Data has been written to CSVs/stage_two/treatment88.csv\n",
      "Parsing Treatment 89\n",
      "Data has been written to CSVs/stage_two/treatment89.csv\n",
      "Parsing Treatment 90\n",
      "Data has been written to CSVs/stage_two/treatment90.csv\n",
      "Parsing Treatment 91\n",
      "Data has been written to CSVs/stage_two/treatment91.csv\n",
      "Parsing Treatment 92\n",
      "Data has been written to CSVs/stage_two/treatment92.csv\n",
      "Parsing Treatment 93\n",
      "Data has been written to CSVs/stage_two/treatment93.csv\n",
      "Parsing Treatment 94\n",
      "Data has been written to CSVs/stage_two/treatment94.csv\n",
      "Parsing Treatment 95\n",
      "Data has been written to CSVs/stage_two/treatment95.csv\n",
      "Parsing Treatment 96\n",
      "Data has been written to CSVs/stage_two/treatment96.csv\n",
      "Parsing Treatment 97\n",
      "Data has been written to CSVs/stage_two/treatment97.csv\n",
      "Parsing Treatment 98\n",
      "Data has been written to CSVs/stage_two/treatment98.csv\n",
      "Parsing Treatment 99\n",
      "Data has been written to CSVs/stage_two/treatment99.csv\n",
      "Parsing Treatment 100\n",
      "Data has been written to CSVs/stage_two/treatment100.csv\n",
      "Parsing Treatment 101\n",
      "Data has been written to CSVs/stage_two/treatment101.csv\n",
      "Parsing Treatment 102\n",
      "Data has been written to CSVs/stage_two/treatment102.csv\n",
      "Parsing Treatment 103\n",
      "Data has been written to CSVs/stage_two/treatment103.csv\n",
      "Parsing Treatment 104\n",
      "Data has been written to CSVs/stage_two/treatment104.csv\n",
      "Parsing Treatment 105\n",
      "Data has been written to CSVs/stage_two/treatment105.csv\n",
      "Parsing Treatment 106\n",
      "Data has been written to CSVs/stage_two/treatment106.csv\n",
      "Parsing Treatment 107\n",
      "Data has been written to CSVs/stage_two/treatment107.csv\n",
      "Parsing Treatment 108\n",
      "Data has been written to CSVs/stage_two/treatment108.csv\n",
      "Parsing Treatment 109\n",
      "Data has been written to CSVs/stage_two/treatment109.csv\n",
      "Parsing Treatment 110\n",
      "Data has been written to CSVs/stage_two/treatment110.csv\n",
      "Parsing Treatment 111\n",
      "Data has been written to CSVs/stage_two/treatment111.csv\n",
      "Parsing Treatment 112\n",
      "Data has been written to CSVs/stage_two/treatment112.csv\n",
      "Parsing Treatment 113\n",
      "Data has been written to CSVs/stage_two/treatment113.csv\n",
      "Parsing Treatment 114\n",
      "Data has been written to CSVs/stage_two/treatment114.csv\n",
      "Parsing Treatment 115\n",
      "Data has been written to CSVs/stage_two/treatment115.csv\n",
      "Parsing Treatment 116\n",
      "Data has been written to CSVs/stage_two/treatment116.csv\n",
      "Parsing Treatment 117\n",
      "Data has been written to CSVs/stage_two/treatment117.csv\n",
      "Parsing Treatment 118\n",
      "Data has been written to CSVs/stage_two/treatment118.csv\n",
      "Parsing Treatment 119\n",
      "Data has been written to CSVs/stage_two/treatment119.csv\n",
      "Parsing Treatment 120\n",
      "Data has been written to CSVs/stage_two/treatment120.csv\n",
      "Parsing Treatment 121\n",
      "Data has been written to CSVs/stage_two/treatment121.csv\n",
      "Parsing Treatment 122\n",
      "Data has been written to CSVs/stage_two/treatment122.csv\n",
      "Parsing Treatment 123\n",
      "Data has been written to CSVs/stage_two/treatment123.csv\n",
      "Parsing Treatment 124\n",
      "Data has been written to CSVs/stage_two/treatment124.csv\n",
      "Parsing Treatment 125\n",
      "Data has been written to CSVs/stage_two/treatment125.csv\n",
      "Parsing Treatment 126\n",
      "Data has been written to CSVs/stage_two/treatment126.csv\n",
      "Parsing Treatment 127\n",
      "Data has been written to CSVs/stage_two/treatment127.csv\n",
      "Parsing Treatment 128\n",
      "Data has been written to CSVs/stage_two/treatment128.csv\n",
      "Parsing Treatment 129\n",
      "Data has been written to CSVs/stage_two/treatment129.csv\n",
      "Parsing Treatment 130\n",
      "Data has been written to CSVs/stage_two/treatment130.csv\n",
      "Parsing Treatment 131\n",
      "Data has been written to CSVs/stage_two/treatment131.csv\n",
      "Parsing Treatment 132\n",
      "Data has been written to CSVs/stage_two/treatment132.csv\n",
      "Parsing Treatment 133\n",
      "Data has been written to CSVs/stage_two/treatment133.csv\n",
      "Parsing Treatment 134\n",
      "Data has been written to CSVs/stage_two/treatment134.csv\n",
      "Parsing Treatment 135\n",
      "Data has been written to CSVs/stage_two/treatment135.csv\n",
      "Parsing Treatment 136\n",
      "Data has been written to CSVs/stage_two/treatment136.csv\n",
      "Parsing Treatment 137\n",
      "Data has been written to CSVs/stage_two/treatment137.csv\n",
      "Parsing Treatment 138\n",
      "Data has been written to CSVs/stage_two/treatment138.csv\n",
      "Parsing Treatment 139\n",
      "Data has been written to CSVs/stage_two/treatment139.csv\n",
      "Parsing Treatment 140\n",
      "Data has been written to CSVs/stage_two/treatment140.csv\n",
      "Parsing Treatment 141\n",
      "Data has been written to CSVs/stage_two/treatment141.csv\n",
      "Parsing Treatment 142\n",
      "Data has been written to CSVs/stage_two/treatment142.csv\n",
      "Parsing Treatment 143\n",
      "Data has been written to CSVs/stage_two/treatment143.csv\n",
      "Parsing Treatment 144\n",
      "Data has been written to CSVs/stage_two/treatment144.csv\n",
      "Parsing Treatment 145\n",
      "Data has been written to CSVs/stage_two/treatment145.csv\n",
      "Parsing Treatment 146\n",
      "Data has been written to CSVs/stage_two/treatment146.csv\n",
      "Parsing Treatment 147\n",
      "Data has been written to CSVs/stage_two/treatment147.csv\n",
      "Parsing Treatment 148\n",
      "Data has been written to CSVs/stage_two/treatment148.csv\n",
      "Parsing Treatment 149\n",
      "Data has been written to CSVs/stage_two/treatment149.csv\n",
      "Parsing Treatment 150\n",
      "Data has been written to CSVs/stage_two/treatment150.csv\n",
      "Parsing Treatment 151\n",
      "Data has been written to CSVs/stage_two/treatment151.csv\n",
      "Parsing Treatment 152\n",
      "Data has been written to CSVs/stage_two/treatment152.csv\n",
      "Parsing Treatment 153\n",
      "Data has been written to CSVs/stage_two/treatment153.csv\n",
      "Parsing Treatment 154\n",
      "Data has been written to CSVs/stage_two/treatment154.csv\n",
      "Parsing Treatment 155\n",
      "Data has been written to CSVs/stage_two/treatment155.csv\n",
      "Parsing Treatment 156\n",
      "Data has been written to CSVs/stage_two/treatment156.csv\n",
      "Parsing Treatment 157\n",
      "Data has been written to CSVs/stage_two/treatment157.csv\n",
      "Parsing Treatment 158\n",
      "Data has been written to CSVs/stage_two/treatment158.csv\n",
      "Parsing Treatment 159\n",
      "Data has been written to CSVs/stage_two/treatment159.csv\n",
      "Parsing Treatment 160\n",
      "Data has been written to CSVs/stage_two/treatment160.csv\n",
      "Parsing Treatment 161\n",
      "Data has been written to CSVs/stage_two/treatment161.csv\n",
      "Parsing Treatment 162\n",
      "Data has been written to CSVs/stage_two/treatment162.csv\n",
      "Parsing Treatment 163\n",
      "Data has been written to CSVs/stage_two/treatment163.csv\n",
      "Parsing Treatment 164\n",
      "Data has been written to CSVs/stage_two/treatment164.csv\n",
      "Parsing Treatment 165\n",
      "Data has been written to CSVs/stage_two/treatment165.csv\n",
      "Parsing Treatment 166\n",
      "Data has been written to CSVs/stage_two/treatment166.csv\n",
      "Parsing Treatment 167\n",
      "Data has been written to CSVs/stage_two/treatment167.csv\n",
      "Parsing Treatment 168\n",
      "Data has been written to CSVs/stage_two/treatment168.csv\n",
      "Parsing Treatment 169\n",
      "Data has been written to CSVs/stage_two/treatment169.csv\n",
      "Parsing Treatment 170\n",
      "Data has been written to CSVs/stage_two/treatment170.csv\n",
      "Parsing Treatment 171\n",
      "Data has been written to CSVs/stage_two/treatment171.csv\n",
      "Parsing Treatment 172\n",
      "Data has been written to CSVs/stage_two/treatment172.csv\n",
      "Parsing Treatment 173\n",
      "Data has been written to CSVs/stage_two/treatment173.csv\n",
      "Parsing Treatment 174\n",
      "Data has been written to CSVs/stage_two/treatment174.csv\n",
      "Parsing Treatment 175\n",
      "Data has been written to CSVs/stage_two/treatment175.csv\n",
      "Parsing Treatment 176\n",
      "Data has been written to CSVs/stage_two/treatment176.csv\n",
      "Parsing Treatment 177\n",
      "Data has been written to CSVs/stage_two/treatment177.csv\n",
      "Parsing Treatment 178\n",
      "Data has been written to CSVs/stage_two/treatment178.csv\n",
      "Parsing Treatment 179\n",
      "Data has been written to CSVs/stage_two/treatment179.csv\n",
      "Parsing Treatment 180\n",
      "Data has been written to CSVs/stage_two/treatment180.csv\n",
      "Parsing Treatment 181\n",
      "Data has been written to CSVs/stage_two/treatment181.csv\n",
      "Parsing Treatment 182\n",
      "Data has been written to CSVs/stage_two/treatment182.csv\n",
      "Parsing Treatment 183\n",
      "Data has been written to CSVs/stage_two/treatment183.csv\n",
      "Parsing Treatment 184\n",
      "Data has been written to CSVs/stage_two/treatment184.csv\n",
      "Parsing Treatment 185\n",
      "Data has been written to CSVs/stage_two/treatment185.csv\n",
      "Parsing Treatment 186\n",
      "Data has been written to CSVs/stage_two/treatment186.csv\n",
      "Parsing Treatment 187\n",
      "Data has been written to CSVs/stage_two/treatment187.csv\n",
      "Parsing Treatment 188\n",
      "Data has been written to CSVs/stage_two/treatment188.csv\n",
      "Parsing Treatment 189\n",
      "Data has been written to CSVs/stage_two/treatment189.csv\n",
      "Parsing Treatment 190\n",
      "Data has been written to CSVs/stage_two/treatment190.csv\n",
      "Parsing Treatment 191\n",
      "Data has been written to CSVs/stage_two/treatment191.csv\n",
      "Parsing Treatment 192\n",
      "Data has been written to CSVs/stage_two/treatment192.csv\n",
      "Parsing Treatment 193\n",
      "Data has been written to CSVs/stage_two/treatment193.csv\n",
      "Parsing Treatment 194\n",
      "Data has been written to CSVs/stage_two/treatment194.csv\n",
      "Parsing Treatment 195\n",
      "Data has been written to CSVs/stage_two/treatment195.csv\n",
      "Parsing Treatment 196\n",
      "Data has been written to CSVs/stage_two/treatment196.csv\n",
      "Parsing Treatment 197\n",
      "Data has been written to CSVs/stage_two/treatment197.csv\n",
      "Parsing Treatment 198\n",
      "Data has been written to CSVs/stage_two/treatment198.csv\n",
      "Parsing Treatment 199\n",
      "Data has been written to CSVs/stage_two/treatment199.csv\n",
      "Parsing Treatment 200\n",
      "Data has been written to CSVs/stage_two/treatment200.csv\n",
      "Parsing Treatment 201\n",
      "Data has been written to CSVs/stage_two/treatment201.csv\n",
      "Parsing Treatment 202\n",
      "Data has been written to CSVs/stage_two/treatment202.csv\n",
      "Parsing Treatment 203\n",
      "Data has been written to CSVs/stage_two/treatment203.csv\n",
      "Parsing Treatment 204\n",
      "Data has been written to CSVs/stage_two/treatment204.csv\n",
      "Parsing Treatment 205\n",
      "Data has been written to CSVs/stage_two/treatment205.csv\n",
      "Parsing Treatment 206\n",
      "Data has been written to CSVs/stage_two/treatment206.csv\n",
      "Parsing Treatment 207\n",
      "Data has been written to CSVs/stage_two/treatment207.csv\n",
      "Parsing Treatment 208\n",
      "Data has been written to CSVs/stage_two/treatment208.csv\n",
      "Parsing Treatment 209\n",
      "Data has been written to CSVs/stage_two/treatment209.csv\n",
      "Parsing Treatment 210\n",
      "Data has been written to CSVs/stage_two/treatment210.csv\n",
      "Parsing Treatment 211\n",
      "Data has been written to CSVs/stage_two/treatment211.csv\n",
      "Parsing Treatment 212\n",
      "Data has been written to CSVs/stage_two/treatment212.csv\n",
      "Parsing Treatment 213\n",
      "Data has been written to CSVs/stage_two/treatment213.csv\n",
      "Parsing Treatment 214\n",
      "Data has been written to CSVs/stage_two/treatment214.csv\n",
      "Parsing Treatment 215\n",
      "Data has been written to CSVs/stage_two/treatment215.csv\n",
      "Parsing Treatment 216\n",
      "Data has been written to CSVs/stage_two/treatment216.csv\n",
      "Parsing Treatment 217\n",
      "Data has been written to CSVs/stage_two/treatment217.csv\n",
      "Parsing Treatment 218\n",
      "Data has been written to CSVs/stage_two/treatment218.csv\n",
      "Parsing Treatment 219\n",
      "Data has been written to CSVs/stage_two/treatment219.csv\n",
      "Parsing Treatment 220\n",
      "Data has been written to CSVs/stage_two/treatment220.csv\n",
      "Parsing Treatment 221\n",
      "Data has been written to CSVs/stage_two/treatment221.csv\n",
      "Parsing Treatment 222\n",
      "Data has been written to CSVs/stage_two/treatment222.csv\n",
      "Parsing Treatment 223\n",
      "Data has been written to CSVs/stage_two/treatment223.csv\n",
      "Parsing Treatment 224\n",
      "Data has been written to CSVs/stage_two/treatment224.csv\n",
      "Parsing Treatment 225\n",
      "Data has been written to CSVs/stage_two/treatment225.csv\n",
      "Parsing Treatment 226\n",
      "Data has been written to CSVs/stage_two/treatment226.csv\n",
      "Parsing Treatment 227\n",
      "Data has been written to CSVs/stage_two/treatment227.csv\n",
      "Parsing Treatment 228\n",
      "Data has been written to CSVs/stage_two/treatment228.csv\n",
      "Parsing Treatment 229\n",
      "Data has been written to CSVs/stage_two/treatment229.csv\n",
      "Parsing Treatment 230\n",
      "Data has been written to CSVs/stage_two/treatment230.csv\n",
      "Parsing Treatment 231\n",
      "Data has been written to CSVs/stage_two/treatment231.csv\n",
      "Parsing Treatment 232\n",
      "Data has been written to CSVs/stage_two/treatment232.csv\n",
      "Parsing Treatment 233\n",
      "Data has been written to CSVs/stage_two/treatment233.csv\n",
      "Parsing Treatment 234\n",
      "Data has been written to CSVs/stage_two/treatment234.csv\n",
      "Parsing Treatment 235\n",
      "Data has been written to CSVs/stage_two/treatment235.csv\n",
      "Parsing Treatment 236\n",
      "Data has been written to CSVs/stage_two/treatment236.csv\n",
      "Parsing Treatment 237\n",
      "Data has been written to CSVs/stage_two/treatment237.csv\n",
      "Parsing Treatment 238\n",
      "Data has been written to CSVs/stage_two/treatment238.csv\n",
      "Parsing Treatment 239\n",
      "Data has been written to CSVs/stage_two/treatment239.csv\n",
      "Parsing Treatment 240\n",
      "Data has been written to CSVs/stage_two/treatment240.csv\n",
      "Parsing Treatment 241\n",
      "Data has been written to CSVs/stage_two/treatment241.csv\n",
      "Parsing Treatment 242\n",
      "Data has been written to CSVs/stage_two/treatment242.csv\n",
      "Parsing Treatment 243\n",
      "Data has been written to CSVs/stage_two/treatment243.csv\n",
      "Parsing Treatment 244\n",
      "Data has been written to CSVs/stage_two/treatment244.csv\n",
      "Parsing Treatment 245\n",
      "Data has been written to CSVs/stage_two/treatment245.csv\n",
      "Parsing Treatment 246\n",
      "Data has been written to CSVs/stage_two/treatment246.csv\n",
      "Parsing Treatment 247\n",
      "Data has been written to CSVs/stage_two/treatment247.csv\n",
      "Parsing Treatment 248\n",
      "Data has been written to CSVs/stage_two/treatment248.csv\n",
      "Parsing Treatment 249\n",
      "Data has been written to CSVs/stage_two/treatment249.csv\n",
      "Parsing Treatment 250\n",
      "Data has been written to CSVs/stage_two/treatment250.csv\n",
      "Parsing Treatment 251\n",
      "Data has been written to CSVs/stage_two/treatment251.csv\n",
      "Parsing Treatment 252\n",
      "Data has been written to CSVs/stage_two/treatment252.csv\n",
      "Parsing Treatment 253\n",
      "Data has been written to CSVs/stage_two/treatment253.csv\n",
      "Parsing Treatment 254\n",
      "Data has been written to CSVs/stage_two/treatment254.csv\n",
      "Parsing Treatment 255\n",
      "Data has been written to CSVs/stage_two/treatment255.csv\n",
      "Parsing Treatment 256\n",
      "Data has been written to CSVs/stage_two/treatment256.csv\n",
      "Parsing Treatment 257\n",
      "Data has been written to CSVs/stage_two/treatment257.csv\n",
      "Parsing Treatment 258\n",
      "Data has been written to CSVs/stage_two/treatment258.csv\n",
      "Parsing Treatment 259\n",
      "Data has been written to CSVs/stage_two/treatment259.csv\n",
      "Parsing Treatment 260\n",
      "Data has been written to CSVs/stage_two/treatment260.csv\n",
      "Parsing Treatment 261\n",
      "Data has been written to CSVs/stage_two/treatment261.csv\n",
      "Parsing Treatment 262\n",
      "Data has been written to CSVs/stage_two/treatment262.csv\n",
      "Parsing Treatment 263\n",
      "Data has been written to CSVs/stage_two/treatment263.csv\n",
      "Parsing Treatment 264\n",
      "Data has been written to CSVs/stage_two/treatment264.csv\n",
      "Parsing Treatment 265\n",
      "Data has been written to CSVs/stage_two/treatment265.csv\n",
      "Parsing Treatment 266\n",
      "Data has been written to CSVs/stage_two/treatment266.csv\n",
      "Parsing Treatment 267\n",
      "Data has been written to CSVs/stage_two/treatment267.csv\n",
      "Parsing Treatment 268\n",
      "Data has been written to CSVs/stage_two/treatment268.csv\n",
      "Parsing Treatment 269\n",
      "Data has been written to CSVs/stage_two/treatment269.csv\n",
      "Parsing Treatment 270\n",
      "Data has been written to CSVs/stage_two/treatment270.csv\n",
      "Parsing Treatment 271\n",
      "Data has been written to CSVs/stage_two/treatment271.csv\n",
      "Parsing Treatment 272\n",
      "Data has been written to CSVs/stage_two/treatment272.csv\n",
      "Parsing Treatment 273\n",
      "Data has been written to CSVs/stage_two/treatment273.csv\n",
      "Parsing Treatment 274\n",
      "Data has been written to CSVs/stage_two/treatment274.csv\n",
      "Parsing Treatment 275\n",
      "Data has been written to CSVs/stage_two/treatment275.csv\n",
      "Parsing Treatment 276\n",
      "Data has been written to CSVs/stage_two/treatment276.csv\n",
      "Parsing Treatment 277\n",
      "Data has been written to CSVs/stage_two/treatment277.csv\n",
      "Parsing Treatment 278\n",
      "Data has been written to CSVs/stage_two/treatment278.csv\n",
      "Parsing Treatment 279\n",
      "Data has been written to CSVs/stage_two/treatment279.csv\n",
      "Parsing Treatment 280\n",
      "Data has been written to CSVs/stage_two/treatment280.csv\n",
      "Parsing Treatment 281\n",
      "Data has been written to CSVs/stage_two/treatment281.csv\n",
      "Parsing Treatment 282\n",
      "Data has been written to CSVs/stage_two/treatment282.csv\n",
      "Parsing Treatment 283\n",
      "Data has been written to CSVs/stage_two/treatment283.csv\n",
      "Parsing Treatment 284\n",
      "Data has been written to CSVs/stage_two/treatment284.csv\n",
      "Parsing Treatment 285\n",
      "Data has been written to CSVs/stage_two/treatment285.csv\n",
      "Parsing Treatment 286\n",
      "Data has been written to CSVs/stage_two/treatment286.csv\n",
      "Parsing Treatment 287\n",
      "Data has been written to CSVs/stage_two/treatment287.csv\n",
      "Parsing Treatment 288\n",
      "Data has been written to CSVs/stage_two/treatment288.csv\n",
      "Parsing Treatment 289\n",
      "Data has been written to CSVs/stage_two/treatment289.csv\n",
      "Parsing Treatment 290\n",
      "Data has been written to CSVs/stage_two/treatment290.csv\n",
      "Parsing Treatment 291\n",
      "Data has been written to CSVs/stage_two/treatment291.csv\n",
      "Parsing Treatment 292\n",
      "Data has been written to CSVs/stage_two/treatment292.csv\n",
      "Parsing Treatment 293\n",
      "Data has been written to CSVs/stage_two/treatment293.csv\n",
      "Parsing Treatment 294\n",
      "Data has been written to CSVs/stage_two/treatment294.csv\n",
      "Parsing Treatment 295\n",
      "Data has been written to CSVs/stage_two/treatment295.csv\n",
      "Parsing Treatment 296\n",
      "Data has been written to CSVs/stage_two/treatment296.csv\n",
      "Parsing Treatment 297\n",
      "Data has been written to CSVs/stage_two/treatment297.csv\n",
      "Parsing Treatment 298\n",
      "Data has been written to CSVs/stage_two/treatment298.csv\n",
      "Parsing Treatment 299\n",
      "Data has been written to CSVs/stage_two/treatment299.csv\n",
      "Parsing Treatment 300\n",
      "Data has been written to CSVs/stage_two/treatment300.csv\n",
      "\n",
      "Parsing complete! Here is the number Web RTC timing errors encountered...\n",
      "___________________________________________________________________________\n",
      "start time errors: 0\n",
      "end time errors: 0\n",
      "missed val errors (off by 1 only): 69\n",
      "missed val errors (off by > 1): [4, 3, 2, 2, 3]\n",
      "extra vals errors (off by 1 only): 23\n",
      "extra vals errors (off by > 1): []\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#file paths for testing_9_Aug\n",
    "file_path_parent_01 = \"testing_stats/testing_9_Aug/treatment\"\n",
    "#file path parent for testing_13_Aug\n",
    "file_path_parent_02 = \"testing_stats/testing_13_Aug/treatment\" \n",
    "#file path parent for testing_27_Aug\n",
    "file_path_parent_03 = \"testing_stats/testing_27_Aug/treatment\"\n",
    "#file path parent for testing_30_Aug\n",
    "file_path_parent_04 = \"testing_stats/testing_30_Aug/treatment\"\n",
    "#file path parent for stage one treatments\n",
    "file_path_parent_05 = \"testing_stats/stage_1/treatment\"\n",
    "#file path parent for stage two treatments\n",
    "file_path_parent_06 = \"testing_stats/stage_2/treatment\"\n",
    "\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 300\n",
    "read_only_status = False\n",
    "verbose_status = False\n",
    "parse_range(file_path_parent_05, lowest_treatment_number, highest_treatment_number, read_only_status, verbose_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
