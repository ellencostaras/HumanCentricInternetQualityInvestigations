{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import ast\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data conversion functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE CONVERSION/CLEANING FUNCTIONS\n",
    "\n",
    "def iso_to_unix_time(iso_string):\n",
    "    '''funtion converting ISO time (like in Web RTC) to unix time'''\n",
    "\n",
    "    dt = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    unix_time = int(dt.timestamp())\n",
    "    return unix_time\n",
    "\n",
    "def convert_to_sec_minus_10_hrs(timestamp):\n",
    "    '''so for some reason Web RTC's timestamps are 10 hours later than the real time\n",
    "    of the call (24-hour time conversion glitch?). so this function converts a millisecond\n",
    "    unix timestamp into seconds, and takes away 10 hours.'''\n",
    "    new_timestamp = math.floor(float(timestamp) / 1000) - 36000\n",
    "    return new_timestamp\n",
    "\n",
    "def separate_by_comma(text_list):\n",
    "    '''Function which takes a list in text form and converts it to a proper Python list'''\n",
    "    \n",
    "    try:\n",
    "        # Use the `ast.literal_eval` method which safely evaluates a string containing\n",
    "        # a Python literal expression (e.g., a list).\n",
    "        parsed_list = ast.literal_eval(text_list)\n",
    "        \n",
    "        # Ensure the parsed output is a list\n",
    "        if isinstance(parsed_list, list):\n",
    "            return parsed_list\n",
    "        else:\n",
    "            raise ValueError(\"Input is not a list\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"Input is not properly formatted or is not a list\")\n",
    "    \n",
    "def parse_audio_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_config = string[char+1:]\n",
    "                break\n",
    "        if \"usedtx\" in codec_config:\n",
    "            dtx = True\n",
    "        else:\n",
    "            dtx = False\n",
    "        if \"useinbandfec\" in codec_config:\n",
    "            fec = True\n",
    "        else:\n",
    "            fec = False\n",
    "        new_codec_info.append((dtx, fec))\n",
    "    \n",
    "    return new_codec_info\n",
    "\n",
    "def parse_video_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_name = string[:char]\n",
    "                break\n",
    "        new_codec_info.append(codec_name)\n",
    "\n",
    "    return new_codec_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(file_path, verbose=False):\n",
    "    '''\n",
    "    This is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats \n",
    "    \n",
    "    This function parses the relevant stats and saves them in custom data types (dictionaries).\n",
    "    '''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    #navigate to where all the stats are stored in the dump\n",
    "    peer_connections = dump.get('PeerConnections', {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get('stats', {})\n",
    "    \n",
    "    #target substrings to pattern match for in stats\n",
    "    target_substrings_IT01V = [\n",
    "        '-[packetsReceived/s]',\n",
    "        '-packetsLost', \n",
    "        '-frameWidth', \n",
    "        '-frameHeight',\n",
    "        '-framesPerSecond', \n",
    "        '-totalFreezesDuration',\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-totalProcessingDelay',\n",
    "        '-jitterBufferDelay',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_IT01A = [\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-jitterBufferDelay',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_OT01V = [\n",
    "        '-[packetsSent/s]',\n",
    "        '-[bytesSent_in_bits/s]',\n",
    "        '-frameWidth',\n",
    "        '-framesPerSecond',\n",
    "        '-totalPacketSendDelay',\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]',\n",
    "        '-qualityLimitationReason',\n",
    "        '-qualityLimitationResolutionChanges',\n",
    "        '-timestamp']\n",
    "    #target_substrings_OT01A = [\n",
    "        #'-[bytesSent_in_bits/s]',\n",
    "        #'-timestamp']\n",
    "    target_substrings_RIV = [\n",
    "        '-roundTripTime',\n",
    "        '-fractionLost',\n",
    "        '-timestamp']\n",
    "    target_substrings_RIA = [\n",
    "        '-fractionLost',\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_ROA = [\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_SV2 = [\n",
    "        '-width',\n",
    "        '-height',\n",
    "        '-framesPerSecond',\n",
    "        '-timestamp']\n",
    "    target_substrings_AP = [\n",
    "        '-totalPlayoutDelay',\n",
    "        '-timestamp']\n",
    "    \n",
    "    #final dictionary data types to store all the values. \n",
    "    #each (None None None) triple will be filled with (values, start time, end time)\n",
    "    target_values_dict_IT01V = {\n",
    "        '-[packetsReceived/s]': (None, None, None),\n",
    "        '-packetsLost': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-frameHeight': (None, None, None),\n",
    "        '-totalFreezesDuration': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-totalProcessingDelay': (None, None, None),\n",
    "        '-jitter': (None, None, None),\n",
    "        '-jitterBufferDelay': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_IT01A = {\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-jitterBufferDelay': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_OT01V = {\n",
    "        '-[packetsSent/s]': (None, None, None),\n",
    "        '-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-totalPacketSendDelay': (None, None, None),\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]': (None, None, None),\n",
    "        '-qualityLimitationReason': (None, None, None),\n",
    "        '-qualityLimitationResolutionChanges': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    #target_values_dict_OT01A = {\n",
    "        #'-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        #'-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIV = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIA = {\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_ROA = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_SV2 = {\n",
    "        '-width': (None, None, None),\n",
    "        '-height': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_AP = {\n",
    "        '-totalPlayoutDelay': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    \n",
    "    #begin searching for the target statistics\n",
    "    for key, value in stats.items():\n",
    "        key_string = str(key)\n",
    "        \n",
    "        # inbound video ones\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            for target_substring in target_substrings_IT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {}) #jump into the innermost dictionary\n",
    "                    if target_values_dict_IT01V[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01V[target_substring] = (parse_video_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "            #special case for finding jitter because it is a substring of other keys too\n",
    "            if key_string[-7:] == '-jitter':\n",
    "                info = stats.get(key, {})\n",
    "                if target_values_dict_IT01V['-jitter'] == (None, None, None):\n",
    "                    target_values_dict_IT01V['-jitter'] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # inbound audio ones\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            for target_substring in target_substrings_IT01A:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_IT01A[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01A[target_substring] = (parse_audio_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "    \n",
    "        # outbound video ones\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            for target_substring in target_substrings_OT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_OT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_OT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # outbound audio ones\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #for target_substring in target_substrings_OT01A:\n",
    "                #if target_substring in key_string:\n",
    "                    #info = stats.get(key, {})\n",
    "                    #if target_values_dict_OT01A[target_substring] == (None, None, None):\n",
    "                        #target_values_dict_OT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # remote inbound video ones\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            for target_substring in target_substrings_RIV:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIV[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIV[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote inbound audio ones\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            for target_substring in target_substrings_RIA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote outbound audio ones\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            for target_substring in target_substrings_ROA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_ROA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_ROA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # video source ones\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            for target_substring in target_substrings_SV2:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_SV2[target_substring] == (None, None, None):\n",
    "                        target_values_dict_SV2[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # audio playout ones\n",
    "        elif key_string[:2] == 'AP':\n",
    "            for target_substring in target_substrings_AP:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_AP[target_substring] == (None, None, None):\n",
    "                        target_values_dict_AP[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "    # Making a global dictionary with unique keys names \n",
    "    single_person_dict = {\n",
    "        'IT01V_packetsRecieved': target_values_dict_IT01V['-[packetsReceived/s]'],\n",
    "        'IT01V_packetsLost': target_values_dict_IT01V['-packetsLost'],\n",
    "        'IT01V_frameWidth': target_values_dict_IT01V['-frameWidth'],\n",
    "        'IT01V_frameHeight': target_values_dict_IT01V['-frameHeight'],\n",
    "        'IT01V_totalFreezesDuration': target_values_dict_IT01V['-totalFreezesDuration'],\n",
    "        'IT01V_framesPerSecond': target_values_dict_IT01V['-framesPerSecond'],\n",
    "        'IT01V_bytesReceived_in_bits/s': target_values_dict_IT01V['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01V_totalProcessingDelay': target_values_dict_IT01V['-totalProcessingDelay'],\n",
    "        'IT01V_jitter': target_values_dict_IT01V['-jitter'],\n",
    "        'IT01V_jitterBufferDelay': target_values_dict_IT01V['-jitterBufferDelay'],\n",
    "        'IT01V_codec': target_values_dict_IT01V['-[codec]'],\n",
    "        'IT01V_timestamps': target_values_dict_IT01V['-timestamp'],\n",
    "        'IT01A_bytesReceived_in_bits/s': target_values_dict_IT01A['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01A_jitterBufferDelay': target_values_dict_IT01A['-jitterBufferDelay'],\n",
    "        'IT01A_(dtx, fec):': target_values_dict_IT01A['-[codec]'],\n",
    "        'IT01A_timestamps': target_values_dict_IT01A['-timestamp'],\n",
    "        'OT01V_packetsSent/s': target_values_dict_OT01V['-[packetsSent/s]'],\n",
    "        'OT01V_bytesSent_in_bits/s': target_values_dict_OT01V['-[bytesSent_in_bits/s]'],\n",
    "        'OT01V_frameWidth': target_values_dict_OT01V['-frameWidth'],\n",
    "        'OT01V_framesPerSecond': target_values_dict_OT01V['-framesPerSecond'],\n",
    "        'OT01V_totalPacketSendDelay': target_values_dict_OT01V['-totalPacketSendDelay'],\n",
    "        'OT01V_totalPacketSendDelay/packetsSent_in_ms': target_values_dict_OT01V['-[totalPacketSendDelay/packetsSent_in_ms]'],\n",
    "        'OT01V_qualityLimitationReason': target_values_dict_OT01V['-qualityLimitationReason'],\n",
    "        'OT01V_qualityLimitationResolutionChanges': target_values_dict_OT01V['-qualityLimitationResolutionChanges'],\n",
    "        'OT01V_timestamps': target_values_dict_OT01V['-timestamp'],\n",
    "        'RIV_roundTripTime': target_values_dict_RIV['-roundTripTime'],\n",
    "        'RIV_fractionLost': target_values_dict_RIV['-fractionLost'],\n",
    "        'RIV_timestamps': target_values_dict_RIV['-timestamp'],\n",
    "        'RIA_fractionLost': target_values_dict_RIA['-fractionLost'],\n",
    "        'RIA_roundTripTime': target_values_dict_RIA['-roundTripTime'],\n",
    "        'RIA_timestamps': target_values_dict_RIA['-timestamp'],\n",
    "        'ROA_roundTripTime': target_values_dict_ROA['-roundTripTime'],\n",
    "        'ROA_timestamps': target_values_dict_ROA['-timestamp'],\n",
    "        'SV2_width': target_values_dict_SV2['-width'],\n",
    "        'SV2_height': target_values_dict_SV2['-height'],\n",
    "        'SV2_framesPerSecond': target_values_dict_SV2['-framesPerSecond'],\n",
    "        'SV2_timestamps': target_values_dict_SV2['-timestamp'],\n",
    "        'AP_totalPlayoutDelay': target_values_dict_AP['-totalPlayoutDelay'],\n",
    "        'AP_timestamps': target_values_dict_AP['-timestamp']}\n",
    "    \n",
    "    if verbose:    \n",
    "        for key, value in single_person_dict.items():\n",
    "            print(key, \": \", value[0])\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return single_person_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and formatting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dictionaries(dict_ellen, dict_aadya):\n",
    "    global_dict = {}\n",
    "    for key, val in dict_ellen.items():\n",
    "        key_string = str(key)\n",
    "        global_dict[key_string + \"_ellen\"] = val\n",
    "        global_dict[key_string + \"_aadya\"] = dict_aadya[key_string]\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "def populate_global_table(global_dict, mistake_tally):\n",
    "\n",
    "    global_start = 999999999999999999999999\n",
    "    global_end = 0\n",
    "    \n",
    "    for key, val in global_dict.items():\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        if start_time < global_start:\n",
    "            global_start = start_time\n",
    "        if end_time > global_end:\n",
    "            global_end = end_time\n",
    "\n",
    "    total_time = global_end - global_start + 1\n",
    "\n",
    "    #populate a rectangular table with -1 for every timestamp\n",
    "    global_table = []\n",
    "    for key, val in global_dict.items():\n",
    "        global_table.append([-1] * total_time)\n",
    "\n",
    "    #truncate timestamps to basic unix timecodes, (round to closest second)\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        if \"timestamps\" in key_string:\n",
    "            old_timestamps = val[0]\n",
    "            new_timestamps = []\n",
    "            for time in old_timestamps:\n",
    "                new_timestamps.append(convert_to_sec_minus_10_hrs(time))\n",
    "            global_dict[key] = (new_timestamps, val[1], val[2])\n",
    "\n",
    "    #replace -1s in the timestamps where data exists for every stat for ellen\n",
    "    row_number = 0\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        person = key_string[-5:]\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            timestamps = global_dict['IT01V_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            timestamps = global_dict['IT01A_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            timestamps = global_dict['OT01V_timestamps_' + person][0]\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #timestamps = global_dict['OT01A_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            timestamps = global_dict['RIV_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            timestamps = global_dict['RIA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            timestamps = global_dict['ROA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            timestamps = global_dict['SV2_timestamps_' + person][0]\n",
    "        elif key_string[:2] == 'AP':\n",
    "            timestamps = global_dict['AP_timestamps_' + person][0]\n",
    "\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        \n",
    "        # Timing error handling:\n",
    "        if start_time < timestamps[0]:\n",
    "            start_time_index = 0 \n",
    "            mistake_tally['start time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        if end_time > timestamps[-1]:\n",
    "            end_time_index = 0\n",
    "            mistake_tally['end time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        \n",
    "        for time in range(len(timestamps)):\n",
    "            if timestamps[time] == start_time:\n",
    "                start_time_index = time\n",
    "            if timestamps[time] == end_time:\n",
    "                end_time_index = time \n",
    "        appropriate_timestamps = timestamps[start_time_index : end_time_index + 1]\n",
    "        \n",
    "        #sneaky cleaning in the cases where Web RTC makes a mistake:\n",
    "        if start_time < timestamps[0]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][difference:], val[1], val[2])\n",
    "        if end_time > timestamps[-1]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][:-difference], val[1], val[2])\n",
    "        \n",
    "        if len(appropriate_timestamps) != len(val[0]):\n",
    "            #print(\"Timing Error found:\", key, \"| len_times:\", len(appropriate_timestamps), \"| len_vals:\", len(val[0]))\n",
    "            difference = len(appropriate_timestamps) - len(val[0])\n",
    "            if difference == 1:\n",
    "                mistake_tally['missed val errors (off by 1 only)'] += 1\n",
    "            elif difference > 1:\n",
    "                mistake_tally['missed val errors (off by > 1)'].append(difference)\n",
    "            elif difference == -1:\n",
    "                mistake_tally['extra vals errors (off by 1 only)'] += 1\n",
    "            elif difference < -1:\n",
    "                mistake_tally['extra vals errors (off by > 1)'].append(-1 * difference)\n",
    "            appropriate_timestamps = appropriate_timestamps[ : -1 * abs(difference)] #bad but neccessary assumption LIMITATION LIMITATION LIMITATION\n",
    "\n",
    "        \n",
    "        for t in range(len(appropriate_timestamps)):\n",
    "            time = appropriate_timestamps[t]\n",
    "            \n",
    "            if val[0][t] >= -1:\n",
    "                global_table[row_number][time - global_start] = val[0][t]  \n",
    "            \n",
    "            else: #handling for the rogue large negative erronous values Web RTC occasionally produces\n",
    "                if (t-1 > 0) and (t+1 < len(val[0]) - 1): #checking we aren't right on the edge of the list\n",
    "                    if (val[0][t-1] == -1) and (val[0][t+1] == -1): #if every second timestep is being recorded\n",
    "                        if (t-2 > 0) and (t-2 < len(val[0]) - 1):\n",
    "                            if (val[0][t-2] != -1) and (val[0][t+2] != -1): #situations like [..., x, -1, error, -1, y, ...]\n",
    "                                left = val[0][t-2]\n",
    "                                right = val[0][t+2]\n",
    "                                if isinstance(left, float) or isinstance(right, float):\n",
    "                                    val[0][t] = (left + right) / 2 #replace the error with the average of it's left-right neighbours\n",
    "                                elif isinstance(left, int):\n",
    "                                    val[0][t] = round((left + right) / 2) #replace the error with the rounded average of it's left-right neighbours\n",
    "                                else:\n",
    "                                    val[0][t] = left #if stat's data are all strings (e.g \"(True, False)\"), replace error with previous entry\n",
    "                    elif (val[0][t-1] != -1) and (val[0][t+1] != -1): #situations like [..., x, error, y, ...]\n",
    "                        left = val[0][t-1]\n",
    "                        right = val[0][t+1]\n",
    "                        if isinstance(left, float) or isinstance(right, float):\n",
    "                            val[0][t] = (left + right) / 2 #replace the error with the average of it's left-right neighbours\n",
    "                        elif isinstance(left, int):\n",
    "                            val[0][t] = round((left + right) / 2) #replace the error with the rounded average of it's left-right neighbours\n",
    "                        else:\n",
    "                            val[0][t] = left #if stat's data are all strings (e.g \"(True, False)\"), replace error with previous entry\n",
    "                    else:\n",
    "                        val[0][t] = -1 #in all other situations, replace with -1 \n",
    "                global_table[row_number][time - global_start] = val[0][t]\n",
    "        \n",
    "        row_number += 1\n",
    "    \n",
    "    return global_table\n",
    "\n",
    "\n",
    "def fix_unwanted_nulls(global_table):\n",
    "    '''function that iterates through a populated global table, finding instances where a stat \n",
    "    only has values recorded once every two seconds, and replaces the empty-second entries with\n",
    "    the average on the values on either side.'''\n",
    "\n",
    "    for stat in global_table:\n",
    "        for i in range(1, len(stat) - 1):\n",
    "            if (stat[i-1] != -1) and (stat[i] == -1) and (stat[i+1] != -1):\n",
    "                \n",
    "                if isinstance(stat[i-1], float) or isinstance(stat[i+1], float):\n",
    "                    stat[i] = (stat[i-1] + stat[i+1]) / 2 #replace the null with the average of it's left-right neighbours\n",
    "                \n",
    "                elif isinstance(stat[i-1], int):\n",
    "                    stat[i] = round((stat[i-1] + stat[i+1]) / 2) #replace the null with the rounded average of it's left-right neighbours\n",
    "                \n",
    "                else:\n",
    "                    stat[i] = stat[i-1] #if stat's data are all strings (e.g \"(True, False)\"), replace null with previous entry\n",
    "\n",
    "\n",
    "def writeout(global_table, global_dict, treatment_number):\n",
    "\n",
    "    # Flip the table (rows -> columns and columns -> rows) for writeout nice-ness\n",
    "    global_table_flipped = []\n",
    "    for col in range(len(global_table[0])):\n",
    "        row_flipped = []\n",
    "        for row in range(len(global_table)):\n",
    "            row_flipped.append(global_table[row][col])\n",
    "        global_table_flipped.append(row_flipped)\n",
    "\n",
    "    # Write out to a CSV\n",
    "    output_file = f\"CSVs/stage_one/treatment{treatment_number}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        header = []\n",
    "        for key, val in global_dict.items():\n",
    "            header.append(key)\n",
    "        writer.writerow(header)\n",
    "        for row in global_table_flipped:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_writeout(parent_file_path, treatment_number, mistake_tally, read_only=False, verbose=False):\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "    dict_ellen = get_stats(file_path_ellen)\n",
    "    dict_aadya = get_stats(file_path_aadya)\n",
    "    global_dict = combine_dictionaries(dict_ellen, dict_aadya)\n",
    "    global_table = populate_global_table(global_dict, mistake_tally)\n",
    "    fix_unwanted_nulls(global_table)\n",
    "    if not read_only:\n",
    "        writeout(global_table, global_dict, treatment_number)\n",
    "\n",
    "def parse_range(parent_file_path, lowest_treatment, highest_treatment, read_only=False, verbose=False):\n",
    "    mistake_tally = {\n",
    "        'start time errors': 0,\n",
    "        'end time errors': 0,\n",
    "        'missed val errors (off by 1 only)': 0,\n",
    "        'missed val errors (off by > 1)': [],\n",
    "        'extra vals errors (off by 1 only)': 0,\n",
    "        'extra vals errors (off by > 1)': []     \n",
    "    }\n",
    "\n",
    "    for i in range(lowest_treatment, highest_treatment + 1):\n",
    "        print(\"Parsing Treatment\", i)\n",
    "        parse_clean_writeout(parent_file_path, i, mistake_tally, read_only, verbose)\n",
    "\n",
    "    print(\"\\nParsing complete! Here is the number Web RTC timing errors encountered...\")\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    for key, val in mistake_tally.items():\n",
    "        print(key + \":\", val)\n",
    "    print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Treatment 1\n",
      "Data has been written to CSVs/stage_one/treatment1.csv\n",
      "Parsing Treatment 2\n",
      "Data has been written to CSVs/stage_one/treatment2.csv\n",
      "Parsing Treatment 3\n",
      "Data has been written to CSVs/stage_one/treatment3.csv\n",
      "Parsing Treatment 4\n",
      "Data has been written to CSVs/stage_one/treatment4.csv\n",
      "Parsing Treatment 5\n",
      "Data has been written to CSVs/stage_one/treatment5.csv\n",
      "Parsing Treatment 6\n",
      "Data has been written to CSVs/stage_one/treatment6.csv\n",
      "Parsing Treatment 7\n",
      "Data has been written to CSVs/stage_one/treatment7.csv\n",
      "Parsing Treatment 8\n",
      "Data has been written to CSVs/stage_one/treatment8.csv\n",
      "Parsing Treatment 9\n",
      "Data has been written to CSVs/stage_one/treatment9.csv\n",
      "Parsing Treatment 10\n",
      "Data has been written to CSVs/stage_one/treatment10.csv\n",
      "Parsing Treatment 11\n",
      "Data has been written to CSVs/stage_one/treatment11.csv\n",
      "Parsing Treatment 12\n",
      "Data has been written to CSVs/stage_one/treatment12.csv\n",
      "Parsing Treatment 13\n",
      "Data has been written to CSVs/stage_one/treatment13.csv\n",
      "Parsing Treatment 14\n",
      "Data has been written to CSVs/stage_one/treatment14.csv\n",
      "Parsing Treatment 15\n",
      "Data has been written to CSVs/stage_one/treatment15.csv\n",
      "\n",
      "Parsing complete! Here is the number Web RTC timing errors encountered...\n",
      "___________________________________________________________________________\n",
      "start time errors: 0\n",
      "end time errors: 0\n",
      "missed val errors (off by 1 only): 4\n",
      "missed val errors (off by > 1): []\n",
      "extra vals errors (off by 1 only): 0\n",
      "extra vals errors (off by > 1): []\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#file paths for testing_9_Aug\n",
    "file_path_parent_01 = \"testing_stats/testing_9_Aug/treatment\"\n",
    "#file path parent for testing_13_Aug\n",
    "file_path_parent_02 = \"testing_stats/testing_13_Aug/treatment\" \n",
    "#file path parent for testing_27_Aug\n",
    "file_path_parent_03 = \"testing_stats/testing_27_Aug/treatment\"\n",
    "#file path parent for testing_30_Aug\n",
    "file_path_parent_04 = \"testing_stats/testing_30_Aug/treatment\"\n",
    "#file path parent for stage one treatments\n",
    "file_path_parent_05 = \"testing_stats/stage_1/treatment\"\n",
    "\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 15\n",
    "read_only_status = False\n",
    "verbose_status = False\n",
    "parse_range(file_path_parent_05, lowest_treatment_number, highest_treatment_number, read_only_status, verbose_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
