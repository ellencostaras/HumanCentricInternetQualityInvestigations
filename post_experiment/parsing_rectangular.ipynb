{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data conversion functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE CONVERSION/CLEANING FUNCTIONS\n",
    "\n",
    "def iso_to_unix_time(iso_string):\n",
    "    '''funtion converting ISO time (like in Web RTC) to unix time'''\n",
    "\n",
    "    dt = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    unix_time = int(dt.timestamp())\n",
    "    return unix_time\n",
    "\n",
    "def convert_to_sec_minus_10_hrs(timestamp):\n",
    "    '''so for some reason Web RTC's timestamps are 10 hours later than the real time\n",
    "    of the call (24-hour time conversion glitch?). so this function converts a millisecond\n",
    "    unix timestamp into seconds, and takes away 10 hours.'''\n",
    "    new_timestamp = math.floor(float(timestamp) / 1000) - 36000\n",
    "    return new_timestamp\n",
    "\n",
    "def separate_by_comma(text_list):\n",
    "    '''Function which takes a list in text form and converts it to a proper Python list'''\n",
    "    \n",
    "    try:\n",
    "        # Use the `ast.literal_eval` method which safely evaluates a string containing\n",
    "        # a Python literal expression (e.g., a list).\n",
    "        parsed_list = ast.literal_eval(text_list)\n",
    "        \n",
    "        # Ensure the parsed output is a list\n",
    "        if isinstance(parsed_list, list):\n",
    "            return parsed_list\n",
    "        else:\n",
    "            raise ValueError(\"Input is not a list\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"Input is not properly formatted or is not a list\")\n",
    "    \n",
    "def parse_audio_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_config = string[char+1:]\n",
    "                break\n",
    "        if \"usedtx\" in codec_config:\n",
    "            dtx = True\n",
    "        else:\n",
    "            dtx = False\n",
    "        if \"useinbandfec\" in codec_config:\n",
    "            fec = True\n",
    "        else:\n",
    "            fec = False\n",
    "        new_codec_info.append((dtx, fec))\n",
    "    \n",
    "    return new_codec_info\n",
    "\n",
    "def parse_video_codec(codec_info):\n",
    "\n",
    "    new_codec_info = []\n",
    "    for string in codec_info:\n",
    "        for char in range(len(string)):\n",
    "            if string[char:char+2] == \" (\":\n",
    "                codec_name = string[:char]\n",
    "                break\n",
    "        new_codec_info.append(codec_name)\n",
    "\n",
    "    return new_codec_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main parsing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(file_path, verbose=False):\n",
    "    '''\n",
    "    This is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats \n",
    "    \n",
    "    This function parses the relevant stats and saves them in custom data types (dictionaries).\n",
    "    '''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    #navigate to where all the stats are stored in the dump\n",
    "    peer_connections = dump.get('PeerConnections', {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get('stats', {})\n",
    "    \n",
    "    #target substrings to pattern match for in stats\n",
    "    target_substrings_IT01V = [\n",
    "        '-[packetsReceived/s]',\n",
    "        '-packetsLost', \n",
    "        '-frameWidth', \n",
    "        '-frameHeight',\n",
    "        '-framesPerSecond', \n",
    "        '-totalFreezesDuration',\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-totalProcessingDelay',\n",
    "        '-jitterBufferDelay',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_IT01A = [\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-jitterBufferDelay',\n",
    "        '-[codec]',\n",
    "        '-timestamp']\n",
    "    target_substrings_OT01V = [\n",
    "        '-[packetsSent/s]',\n",
    "        '-[bytesSent_in_bits/s]',\n",
    "        '-frameWidth',\n",
    "        '-framesPerSecond',\n",
    "        '-totalPacketSendDelay',\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]',\n",
    "        '-qualityLimitationReason',\n",
    "        '-qualityLimitationResolutionChanges',\n",
    "        '-timestamp']\n",
    "    #target_substrings_OT01A = [\n",
    "        #'-[bytesSent_in_bits/s]',\n",
    "        #'-timestamp']\n",
    "    target_substrings_RIV = [\n",
    "        '-roundTripTime',\n",
    "        '-fractionLost',\n",
    "        '-timestamp']\n",
    "    target_substrings_RIA = [\n",
    "        '-fractionLost',\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_ROA = [\n",
    "        '-roundTripTime',\n",
    "        '-timestamp']\n",
    "    target_substrings_SV2 = [\n",
    "        '-width',\n",
    "        '-height',\n",
    "        '-framesPerSecond',\n",
    "        '-timestamp']\n",
    "    target_substrings_AP = [\n",
    "        '-totalPlayoutDelay',\n",
    "        '-timestamp']\n",
    "    \n",
    "    #final dictionary data types to store all the values. \n",
    "    #each (None None None) triple will be filled with (values, start time, end time)\n",
    "    target_values_dict_IT01V = {\n",
    "        '-[packetsReceived/s]': (None, None, None),\n",
    "        '-packetsLost': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-frameHeight': (None, None, None),\n",
    "        '-totalFreezesDuration': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-totalProcessingDelay': (None, None, None),\n",
    "        '-jitter': (None, None, None),\n",
    "        '-jitterBufferDelay': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_IT01A = {\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-jitterBufferDelay': (None, None, None),\n",
    "        '-[codec]': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_OT01V = {\n",
    "        '-[packetsSent/s]': (None, None, None),\n",
    "        '-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-totalPacketSendDelay': (None, None, None),\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]': (None, None, None),\n",
    "        '-qualityLimitationReason': (None, None, None),\n",
    "        '-qualityLimitationResolutionChanges': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    #target_values_dict_OT01A = {\n",
    "        #'-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        #'-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIV = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_RIA = {\n",
    "        '-fractionLost': (None, None, None),\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_ROA = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_SV2 = {\n",
    "        '-width': (None, None, None),\n",
    "        '-height': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    target_values_dict_AP = {\n",
    "        '-totalPlayoutDelay': (None, None, None),\n",
    "        '-timestamp': (None, None, None)}\n",
    "    \n",
    "    #begin searching for the target statistics\n",
    "    for key, value in stats.items():\n",
    "        key_string = str(key)\n",
    "        \n",
    "        # inbound video ones\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            for target_substring in target_substrings_IT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {}) #jump into the innermost dictionary\n",
    "                    if target_values_dict_IT01V[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01V[target_substring] = (parse_video_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "            #special case for finding jitter because it is a substring of other keys too\n",
    "            if key_string[-7:] == '-jitter':\n",
    "                info = stats.get(key, {})\n",
    "                if target_values_dict_IT01V['-jitter'] == (None, None, None):\n",
    "                    target_values_dict_IT01V['-jitter'] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # inbound audio ones\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            for target_substring in target_substrings_IT01A:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_IT01A[target_substring] == (None, None, None):\n",
    "                        if target_substring == \"-[codec]\":\n",
    "                            target_values_dict_IT01A[target_substring] = (parse_audio_codec(separate_by_comma(info['values'])), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                        else:\n",
    "                            target_values_dict_IT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "    \n",
    "        # outbound video ones\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            for target_substring in target_substrings_OT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_OT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_OT01V[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # outbound audio ones\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #for target_substring in target_substrings_OT01A:\n",
    "                #if target_substring in key_string:\n",
    "                    #info = stats.get(key, {})\n",
    "                    #if target_values_dict_OT01A[target_substring] == (None, None, None):\n",
    "                        #target_values_dict_OT01A[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "        # remote inbound video ones\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            for target_substring in target_substrings_RIV:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIV[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIV[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote inbound audio ones\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            for target_substring in target_substrings_RIA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "        \n",
    "        # remote outbound audio ones\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            for target_substring in target_substrings_ROA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_ROA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_ROA[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # video source ones\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            for target_substring in target_substrings_SV2:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_SV2[target_substring] == (None, None, None):\n",
    "                        target_values_dict_SV2[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "                    \n",
    "        # audio playout ones\n",
    "        elif key_string[:2] == 'AP':\n",
    "            for target_substring in target_substrings_AP:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_AP[target_substring] == (None, None, None):\n",
    "                        target_values_dict_AP[target_substring] = (separate_by_comma(info['values']), iso_to_unix_time(info['startTime']), iso_to_unix_time(info['endTime']))\n",
    "\n",
    "    # Making a global dictionary with unique keys names \n",
    "    single_person_dict = {\n",
    "        'IT01V_packetsRecieved': target_values_dict_IT01V['-[packetsReceived/s]'],\n",
    "        'IT01V_packetsLost': target_values_dict_IT01V['-packetsLost'],\n",
    "        'IT01V_frameWidth': target_values_dict_IT01V['-frameWidth'],\n",
    "        'IT01V_frameHeight': target_values_dict_IT01V['-frameHeight'],\n",
    "        'IT01V_totalFreezesDuration': target_values_dict_IT01V['-totalFreezesDuration'],\n",
    "        'IT01V_framesPerSecond': target_values_dict_IT01V['-framesPerSecond'],\n",
    "        'IT01V_bytesReceived_in_bits/s': target_values_dict_IT01V['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01V_totalProcessingDelay': target_values_dict_IT01V['-totalProcessingDelay'],\n",
    "        'IT01V_jitter': target_values_dict_IT01V['-jitter'],\n",
    "        'IT01V_jitterBufferDelay': target_values_dict_IT01V['-jitterBufferDelay'],\n",
    "        'IT01V_codec': target_values_dict_IT01V['-[codec]'],\n",
    "        'IT01V_timestamps': target_values_dict_IT01V['-timestamp'],\n",
    "        'IT01A_bytesReceived_in_bits/s': target_values_dict_IT01A['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01A_jitterBufferDelay': target_values_dict_IT01A['-jitterBufferDelay'],\n",
    "        'IT01A_(dtx, fec):': target_values_dict_IT01A['-[codec]'],\n",
    "        'IT01A_timestamps': target_values_dict_IT01A['-timestamp'],\n",
    "        'OT01V_packetsSent/s': target_values_dict_OT01V['-[packetsSent/s]'],\n",
    "        'OT01V_bytesSent_in_bits/s': target_values_dict_OT01V['-[bytesSent_in_bits/s]'],\n",
    "        'OT01V_frameWidth': target_values_dict_OT01V['-frameWidth'],\n",
    "        'OT01V_framesPerSecond': target_values_dict_OT01V['-framesPerSecond'],\n",
    "        'OT01V_totalPacketSendDelay': target_values_dict_OT01V['-totalPacketSendDelay'],\n",
    "        'OT01V_totalPacketSendDelay/packetsSent_in_ms': target_values_dict_OT01V['-[totalPacketSendDelay/packetsSent_in_ms]'],\n",
    "        'OT01V_qualityLimitationReason': target_values_dict_OT01V['-qualityLimitationReason'],\n",
    "        'OT01V_qualityLimitationResolutionChanges': target_values_dict_OT01V['-qualityLimitationResolutionChanges'],\n",
    "        'OT01V_timestamps': target_values_dict_OT01V['-timestamp'],\n",
    "        'RIV_roundTripTime': target_values_dict_RIV['-roundTripTime'],\n",
    "        'RIV_fractionLost': target_values_dict_RIV['-fractionLost'],\n",
    "        'RIV_timestamps': target_values_dict_RIV['-timestamp'],\n",
    "        'RIA_fractionLost': target_values_dict_RIA['-fractionLost'],\n",
    "        'RIA_roundTripTime': target_values_dict_RIA['-roundTripTime'],\n",
    "        'RIA_timestamps': target_values_dict_RIA['-timestamp'],\n",
    "        'ROA_roundTripTime': target_values_dict_ROA['-roundTripTime'],\n",
    "        'ROA_timestamps': target_values_dict_ROA['-timestamp'],\n",
    "        'SV2_width': target_values_dict_SV2['-width'],\n",
    "        'SV2_height': target_values_dict_SV2['-height'],\n",
    "        'SV2_framesPerSecond': target_values_dict_SV2['-framesPerSecond'],\n",
    "        'SV2_timestamps': target_values_dict_SV2['-timestamp'],\n",
    "        'AP_totalPlayoutDelay': target_values_dict_AP['-totalPlayoutDelay'],\n",
    "        'AP_timestamps': target_values_dict_AP['-timestamp']}\n",
    "    \n",
    "    if verbose:    \n",
    "        for key, value in single_person_dict.items():\n",
    "            print(key, \": \", value[0])\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        \n",
    "    return single_person_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and formatting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dictionaries(dict_ellen, dict_aadya):\n",
    "    global_dict = {}\n",
    "    for key, val in dict_ellen.items():\n",
    "        key_string = str(key)\n",
    "        global_dict[key_string + \"_ellen\"] = val\n",
    "        global_dict[key_string + \"_aadya\"] = dict_aadya[key_string]\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "def populate_global_table(global_dict, mistake_tally):\n",
    "\n",
    "    global_start = 999999999999999999999999\n",
    "    global_end = 0\n",
    "    \n",
    "    for key, val in global_dict.items():\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        if start_time < global_start:\n",
    "            global_start = start_time\n",
    "        if end_time > global_end:\n",
    "            global_end = end_time\n",
    "\n",
    "    total_time = global_end - global_start + 1\n",
    "\n",
    "    #populate a rectangular table with -1 for every timestamp\n",
    "    global_table = []\n",
    "    for key, val in global_dict.items():\n",
    "        global_table.append([-1] * total_time)\n",
    "\n",
    "    #truncate timestamps to basic unix timecodes, (round to closest second)\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        if \"timestamps\" in key_string:\n",
    "            old_timestamps = val[0]\n",
    "            new_timestamps = []\n",
    "            for time in old_timestamps:\n",
    "                new_timestamps.append(convert_to_sec_minus_10_hrs(time))\n",
    "            global_dict[key] = (new_timestamps, val[1], val[2])\n",
    "\n",
    "    #replace -1s in the timestamps where data exists for every stat for ellen\n",
    "    row_number = 0\n",
    "    for key, val in global_dict.items():\n",
    "        key_string = str(key)\n",
    "        person = key_string[-5:]\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            timestamps = global_dict['IT01V_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            timestamps = global_dict['IT01A_timestamps_' + person][0]\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            timestamps = global_dict['OT01V_timestamps_' + person][0]\n",
    "        #elif key_string[:5] == 'OT01A':\n",
    "            #timestamps = global_dict['OT01A_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            timestamps = global_dict['RIV_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            timestamps = global_dict['RIA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            timestamps = global_dict['ROA_timestamps_' + person][0]\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            timestamps = global_dict['SV2_timestamps_' + person][0]\n",
    "        elif key_string[:2] == 'AP':\n",
    "            timestamps = global_dict['AP_timestamps_' + person][0]\n",
    "\n",
    "        start_time = val[1]\n",
    "        end_time = val[2]\n",
    "        \n",
    "        # Timing error handling:\n",
    "        if start_time < timestamps[0]:\n",
    "            start_time_index = 0 \n",
    "            mistake_tally['start time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        if end_time > timestamps[-1]:\n",
    "            end_time_index = 0\n",
    "            mistake_tally['end time errors'] += 1\n",
    "        else:\n",
    "            start_time_index = None\n",
    "        \n",
    "        for time in range(len(timestamps)):\n",
    "            if timestamps[time] == start_time:\n",
    "                start_time_index = time\n",
    "            if timestamps[time] == end_time:\n",
    "                end_time_index = time \n",
    "        appropriate_timestamps = timestamps[start_time_index : end_time_index + 1]\n",
    "        \n",
    "        #sneaky cleaning in the cases where Web RTC makes a mistake:\n",
    "        if start_time < timestamps[0]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][difference:], val[1], val[2])\n",
    "        if end_time > timestamps[-1]:\n",
    "            if len(val[0]) > len(appropriate_timestamps):\n",
    "                difference = len(val[0]) - len(appropriate_timestamps)\n",
    "            val = (val[0][:-difference], val[1], val[2])\n",
    "        \n",
    "        if len(appropriate_timestamps) != len(val[0]):\n",
    "            #print(\"Timing Error found:\", key, \"| len_times:\", len(appropriate_timestamps), \"| len_vals:\", len(val[0]))\n",
    "            difference = len(appropriate_timestamps) - len(val[0])\n",
    "            if difference == 1:\n",
    "                mistake_tally['missed val errors (off by 1 only)'] += 1\n",
    "            elif difference > 1:\n",
    "                mistake_tally['missed val errors (off by > 1)'].append(difference)\n",
    "            elif difference == -1:\n",
    "                mistake_tally['extra vals errors (off by 1 only)'] += 1\n",
    "            elif difference < -1:\n",
    "                mistake_tally['extra vals errors (off by > 1)'].append(-1 * difference)\n",
    "            appropriate_timestamps = appropriate_timestamps[ : -1 * abs(difference)] #bad but neccessary assumption LIMITATION LIMITATION LIMITATION\n",
    "\n",
    "        \n",
    "        for t in range(len(appropriate_timestamps)):\n",
    "            time = appropriate_timestamps[t]\n",
    "            global_table[row_number][time - global_start] = val[0][t]\n",
    "        row_number += 1\n",
    "    \n",
    "    return global_table\n",
    "\n",
    "\n",
    "def writeout(global_table, global_dict, treatment_number):\n",
    "\n",
    "    # Flip the table (rows -> columns and columns -> rows) for writeout nice-ness\n",
    "    global_table_flipped = []\n",
    "    for col in range(len(global_table[0])):\n",
    "        row_flipped = []\n",
    "        for row in range(len(global_table)):\n",
    "            row_flipped.append(global_table[row][col])\n",
    "        global_table_flipped.append(row_flipped)\n",
    "\n",
    "    # Write out to a CSV\n",
    "    output_file = f\"CSVs/stage_one/treatment{treatment_number}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        header = []\n",
    "        for key, val in global_dict.items():\n",
    "            header.append(key)\n",
    "        writer.writerow(header)\n",
    "        for row in global_table_flipped:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_writeout(parent_file_path, treatment_number, mistake_tally, read_only=False, verbose=False):\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "    dict_ellen = get_stats(file_path_ellen)\n",
    "    dict_aadya = get_stats(file_path_aadya)\n",
    "    global_dict = combine_dictionaries(dict_ellen, dict_aadya)\n",
    "    global_table = populate_global_table(global_dict, mistake_tally)\n",
    "    if not read_only:\n",
    "        writeout(global_table, global_dict, treatment_number)\n",
    "\n",
    "def parse_range(parent_file_path, lowest_treatment, highest_treatment, read_only=False, verbose=False):\n",
    "    mistake_tally = {\n",
    "        'start time errors': 0,\n",
    "        'end time errors': 0,\n",
    "        'missed val errors (off by 1 only)': 0,\n",
    "        'missed val errors (off by > 1)': [],\n",
    "        'extra vals errors (off by 1 only)': 0,\n",
    "        'extra vals errors (off by > 1)': []     \n",
    "    }\n",
    "\n",
    "    for i in range(lowest_treatment, highest_treatment + 1):\n",
    "        print(\"Parsing Treatment\", i)\n",
    "        parse_clean_writeout(parent_file_path, i, mistake_tally, read_only, verbose)\n",
    "\n",
    "    print(\"\\nParsing complete! Here is the number Web RTC timing errors encountered...\")\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    for key, val in mistake_tally.items():\n",
    "        print(key + \":\", val)\n",
    "    print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Treatment 1\n",
      "Parsing Treatment 2\n",
      "Parsing Treatment 3\n",
      "Parsing Treatment 4\n",
      "Parsing Treatment 5\n",
      "Parsing Treatment 6\n",
      "Parsing Treatment 7\n",
      "Parsing Treatment 8\n",
      "Parsing Treatment 9\n",
      "Parsing Treatment 10\n",
      "Parsing Treatment 11\n",
      "Parsing Treatment 12\n",
      "Parsing Treatment 13\n",
      "Parsing Treatment 14\n",
      "Parsing Treatment 15\n",
      "Parsing Treatment 16\n",
      "Parsing Treatment 17\n",
      "Parsing Treatment 18\n",
      "Parsing Treatment 19\n",
      "Parsing Treatment 20\n",
      "Parsing Treatment 21\n",
      "Parsing Treatment 22\n",
      "Parsing Treatment 23\n",
      "Parsing Treatment 24\n",
      "Parsing Treatment 25\n",
      "Parsing Treatment 26\n",
      "Parsing Treatment 27\n",
      "Parsing Treatment 28\n",
      "Parsing Treatment 29\n",
      "Parsing Treatment 30\n",
      "Parsing Treatment 31\n",
      "Parsing Treatment 32\n",
      "Parsing Treatment 33\n",
      "Parsing Treatment 34\n",
      "Parsing Treatment 35\n",
      "Parsing Treatment 36\n",
      "Parsing Treatment 37\n",
      "Parsing Treatment 38\n",
      "Parsing Treatment 39\n",
      "Parsing Treatment 40\n",
      "Parsing Treatment 41\n",
      "Parsing Treatment 42\n",
      "Parsing Treatment 43\n",
      "Parsing Treatment 44\n",
      "Parsing Treatment 45\n",
      "Parsing Treatment 46\n",
      "Parsing Treatment 47\n",
      "Parsing Treatment 48\n",
      "Parsing Treatment 49\n",
      "Parsing Treatment 50\n",
      "Parsing Treatment 51\n",
      "Parsing Treatment 52\n",
      "Parsing Treatment 53\n",
      "Parsing Treatment 54\n",
      "Parsing Treatment 55\n",
      "Parsing Treatment 56\n",
      "Parsing Treatment 57\n",
      "Parsing Treatment 58\n",
      "Parsing Treatment 59\n",
      "Parsing Treatment 60\n",
      "Parsing Treatment 61\n",
      "Parsing Treatment 62\n",
      "Parsing Treatment 63\n",
      "Parsing Treatment 64\n",
      "Parsing Treatment 65\n",
      "Parsing Treatment 66\n",
      "Parsing Treatment 67\n",
      "Parsing Treatment 68\n",
      "Parsing Treatment 69\n",
      "Parsing Treatment 70\n",
      "Parsing Treatment 71\n",
      "Parsing Treatment 72\n",
      "Parsing Treatment 73\n",
      "Parsing Treatment 74\n",
      "Parsing Treatment 75\n",
      "Parsing Treatment 76\n",
      "Parsing Treatment 77\n",
      "Parsing Treatment 78\n",
      "Parsing Treatment 79\n",
      "Parsing Treatment 80\n",
      "Parsing Treatment 81\n",
      "Parsing Treatment 82\n",
      "Parsing Treatment 83\n",
      "Parsing Treatment 84\n",
      "Parsing Treatment 85\n",
      "Parsing Treatment 86\n",
      "Parsing Treatment 87\n",
      "Parsing Treatment 88\n",
      "Parsing Treatment 89\n",
      "Parsing Treatment 90\n",
      "Parsing Treatment 91\n",
      "Parsing Treatment 92\n",
      "Parsing Treatment 93\n",
      "Parsing Treatment 94\n",
      "Parsing Treatment 95\n",
      "Parsing Treatment 96\n",
      "Parsing Treatment 97\n",
      "Parsing Treatment 98\n",
      "Parsing Treatment 99\n",
      "Parsing Treatment 100\n",
      "Parsing Treatment 101\n",
      "Parsing Treatment 102\n",
      "Timing Error found: IT01V_framesPerSecond_aadya | len_times: 26 | len_vals: 22\n",
      "Parsing Treatment 103\n",
      "Parsing Treatment 104\n",
      "Parsing Treatment 105\n",
      "Parsing Treatment 106\n",
      "Parsing Treatment 107\n",
      "Parsing Treatment 108\n",
      "Parsing Treatment 109\n",
      "Parsing Treatment 110\n",
      "Parsing Treatment 111\n",
      "Parsing Treatment 112\n",
      "Timing Error found: IT01V_framesPerSecond_ellen | len_times: 29 | len_vals: 26\n",
      "Parsing Treatment 113\n",
      "Parsing Treatment 114\n",
      "Parsing Treatment 115\n",
      "Parsing Treatment 116\n",
      "Parsing Treatment 117\n",
      "Parsing Treatment 118\n",
      "Parsing Treatment 119\n",
      "Parsing Treatment 120\n",
      "Timing Error found: OT01V_framesPerSecond_ellen | len_times: 29 | len_vals: 27\n",
      "Parsing Treatment 121\n",
      "Parsing Treatment 122\n",
      "Parsing Treatment 123\n",
      "Parsing Treatment 124\n",
      "Parsing Treatment 125\n",
      "Parsing Treatment 126\n",
      "Parsing Treatment 127\n",
      "Parsing Treatment 128\n",
      "Parsing Treatment 129\n",
      "Parsing Treatment 130\n",
      "Parsing Treatment 131\n",
      "Parsing Treatment 132\n",
      "Parsing Treatment 133\n",
      "Parsing Treatment 134\n",
      "Parsing Treatment 135\n",
      "Parsing Treatment 136\n",
      "Parsing Treatment 137\n",
      "Parsing Treatment 138\n",
      "Parsing Treatment 139\n",
      "Parsing Treatment 140\n",
      "Parsing Treatment 141\n",
      "Parsing Treatment 142\n",
      "Parsing Treatment 143\n",
      "Parsing Treatment 144\n",
      "Parsing Treatment 145\n",
      "Parsing Treatment 146\n",
      "Parsing Treatment 147\n",
      "Parsing Treatment 148\n",
      "Parsing Treatment 149\n",
      "Parsing Treatment 150\n",
      "Parsing Treatment 151\n",
      "Parsing Treatment 152\n",
      "Parsing Treatment 153\n",
      "Parsing Treatment 154\n",
      "Parsing Treatment 155\n",
      "Parsing Treatment 156\n",
      "Parsing Treatment 157\n",
      "Parsing Treatment 158\n",
      "Parsing Treatment 159\n",
      "Parsing Treatment 160\n",
      "Parsing Treatment 161\n",
      "Parsing Treatment 162\n",
      "Parsing Treatment 163\n",
      "Parsing Treatment 164\n",
      "Parsing Treatment 165\n",
      "Parsing Treatment 166\n",
      "Parsing Treatment 167\n",
      "Parsing Treatment 168\n",
      "Parsing Treatment 169\n",
      "Parsing Treatment 170\n",
      "Parsing Treatment 171\n",
      "Parsing Treatment 172\n",
      "Parsing Treatment 173\n",
      "Parsing Treatment 174\n",
      "Parsing Treatment 175\n",
      "Parsing Treatment 176\n",
      "Parsing Treatment 177\n",
      "Parsing Treatment 178\n",
      "Parsing Treatment 179\n",
      "Parsing Treatment 180\n",
      "Parsing Treatment 181\n",
      "Parsing Treatment 182\n",
      "Parsing Treatment 183\n",
      "Parsing Treatment 184\n",
      "Parsing Treatment 185\n",
      "Parsing Treatment 186\n",
      "Parsing Treatment 187\n",
      "Parsing Treatment 188\n",
      "Parsing Treatment 189\n",
      "Parsing Treatment 190\n",
      "Parsing Treatment 191\n",
      "Parsing Treatment 192\n",
      "Parsing Treatment 193\n",
      "Parsing Treatment 194\n",
      "Parsing Treatment 195\n",
      "Parsing Treatment 196\n",
      "Parsing Treatment 197\n",
      "Parsing Treatment 198\n",
      "Parsing Treatment 199\n",
      "Parsing Treatment 200\n",
      "Parsing Treatment 201\n",
      "Parsing Treatment 202\n",
      "Parsing Treatment 203\n",
      "Parsing Treatment 204\n",
      "Parsing Treatment 205\n",
      "Parsing Treatment 206\n",
      "Parsing Treatment 207\n",
      "Parsing Treatment 208\n",
      "Parsing Treatment 209\n",
      "Parsing Treatment 210\n",
      "Parsing Treatment 211\n",
      "Parsing Treatment 212\n",
      "Parsing Treatment 213\n",
      "Parsing Treatment 214\n",
      "Parsing Treatment 215\n",
      "Timing Error found: IT01V_framesPerSecond_aadya | len_times: 28 | len_vals: 26\n",
      "Parsing Treatment 216\n",
      "Parsing Treatment 217\n",
      "Parsing Treatment 218\n",
      "Parsing Treatment 219\n",
      "Parsing Treatment 220\n",
      "Parsing Treatment 221\n",
      "Parsing Treatment 222\n",
      "Parsing Treatment 223\n",
      "Parsing Treatment 224\n",
      "Parsing Treatment 225\n",
      "Parsing Treatment 226\n",
      "Parsing Treatment 227\n",
      "Parsing Treatment 228\n",
      "Parsing Treatment 229\n",
      "Parsing Treatment 230\n",
      "Timing Error found: IT01V_framesPerSecond_ellen | len_times: 28 | len_vals: 25\n",
      "Parsing Treatment 231\n",
      "Parsing Treatment 232\n",
      "Parsing Treatment 233\n",
      "Parsing Treatment 234\n",
      "Parsing Treatment 235\n",
      "Parsing Treatment 236\n",
      "Parsing Treatment 237\n",
      "Parsing Treatment 238\n",
      "Parsing Treatment 239\n",
      "Parsing Treatment 240\n",
      "Parsing Treatment 241\n",
      "Parsing Treatment 242\n",
      "Parsing Treatment 243\n",
      "Parsing Treatment 244\n",
      "Parsing Treatment 245\n",
      "Parsing Treatment 246\n",
      "Parsing Treatment 247\n",
      "Parsing Treatment 248\n",
      "Parsing Treatment 249\n",
      "Parsing Treatment 250\n",
      "Parsing Treatment 251\n",
      "Parsing Treatment 252\n",
      "Parsing Treatment 253\n",
      "Parsing Treatment 254\n",
      "Parsing Treatment 255\n",
      "Parsing Treatment 256\n",
      "Parsing Treatment 257\n",
      "Parsing Treatment 258\n",
      "Parsing Treatment 259\n",
      "Parsing Treatment 260\n",
      "Parsing Treatment 261\n",
      "Parsing Treatment 262\n",
      "Parsing Treatment 263\n",
      "Parsing Treatment 264\n",
      "Parsing Treatment 265\n",
      "Parsing Treatment 266\n",
      "Parsing Treatment 267\n",
      "Parsing Treatment 268\n",
      "Parsing Treatment 269\n",
      "Parsing Treatment 270\n",
      "Parsing Treatment 271\n",
      "Parsing Treatment 272\n",
      "Parsing Treatment 273\n",
      "Parsing Treatment 274\n",
      "Parsing Treatment 275\n",
      "Parsing Treatment 276\n",
      "Parsing Treatment 277\n",
      "Parsing Treatment 278\n",
      "Parsing Treatment 279\n",
      "Parsing Treatment 280\n",
      "Parsing Treatment 281\n",
      "Parsing Treatment 282\n",
      "Parsing Treatment 283\n",
      "Parsing Treatment 284\n",
      "Parsing Treatment 285\n",
      "Parsing Treatment 286\n",
      "Parsing Treatment 287\n",
      "Parsing Treatment 288\n",
      "Parsing Treatment 289\n",
      "Parsing Treatment 290\n",
      "Parsing Treatment 291\n",
      "Parsing Treatment 292\n",
      "Parsing Treatment 293\n",
      "Parsing Treatment 294\n",
      "Parsing Treatment 295\n",
      "Parsing Treatment 296\n",
      "Parsing Treatment 297\n",
      "Parsing Treatment 298\n",
      "Parsing Treatment 299\n",
      "Parsing Treatment 300\n",
      "\n",
      "Parsing complete! Here is the number Web RTC timing errors encountered...\n",
      "___________________________________________________________________________\n",
      "start time errors: 0\n",
      "end time errors: 0\n",
      "missed val errors (off by 1 only): 69\n",
      "missed val errors (off by > 1): [4, 3, 2, 2, 3]\n",
      "extra vals errors (off by 1 only): 23\n",
      "extra vals errors (off by > 1): []\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#file paths for testing_9_Aug\n",
    "file_path_parent_01 = \"testing_stats/testing_9_Aug/treatment\"\n",
    "#file path parent for testing_13_Aug\n",
    "file_path_parent_02 = \"testing_stats/testing_13_Aug/treatment\" \n",
    "#file path parent for testing_27_Aug\n",
    "file_path_parent_03 = \"testing_stats/testing_27_Aug/treatment\"\n",
    "#file path parent for testing_30_Aug\n",
    "file_path_parent_04 = \"testing_stats/testing_30_Aug/treatment\"\n",
    "#file path parent for stage one treatments\n",
    "file_path_parent_05 = \"testing_stats/stage_1/treatment\"\n",
    "\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 300\n",
    "read_only_status = True\n",
    "verbose_status = False\n",
    "parse_range(file_path_parent_05, lowest_treatment_number, highest_treatment_number, read_only_status, verbose_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
