{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Finding all the stat names in the Web RTC dump ----------------\n",
    "\n",
    "def get_key_names(file_path):\n",
    "    \n",
    "    '''\n",
    "    TThis is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats  \n",
    "    \n",
    "    This function exists just to print the names of all the statistics possible to select from. \n",
    "    '''\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    peer_connections = dump.get(\"PeerConnections\", {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get(\"stats\", {})\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------ Parsing relevant stats -----------------------------\n",
    "\n",
    "def get_call_identifiers(file_path):\n",
    "    '''May need to implement this later for the real experiment'''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "        \n",
    "        #user_media = dump.get('getUserMedia', [])\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_stats(file_path, verbose=False):\n",
    "    '''\n",
    "    This is the nested dictionary structure in the json .txt dump:\n",
    "    dump_file_name -> PeerConnections -> the 3rd dictionary (alphanumeric code) -> stats \n",
    "    \n",
    "    This function parses the relevant stats and saves them in custom data types (dictionaries).\n",
    "    '''\n",
    "    \n",
    "    #opening the dump .txt JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dump = json.load(file)\n",
    "    \n",
    "    #navigate to where all the stats are stored in the dump\n",
    "    peer_connections = dump.get('PeerConnections', {})\n",
    "    keys_list = list(peer_connections.keys())\n",
    "    third_dictionary = peer_connections.get(keys_list[-1], {})\n",
    "    stats = third_dictionary.get('stats', {})\n",
    "    \n",
    "    #target substrings to pattern match for in stats\n",
    "    target_substrings_IT01V = [\n",
    "        '-[packetsReceived/s]',\n",
    "        '-packetsLost', \n",
    "        '-frameWidth', \n",
    "        '-framesPerSecond', \n",
    "        '-totalFreezesDuration',\n",
    "        '-[bytesReceived_in_bits/s]',\n",
    "        '-totalProcessingDelay']\n",
    "    target_substrings_IT01A = [\n",
    "        '-[bytesReceived_in_bits/s]']\n",
    "    target_substrings_OT01V = [\n",
    "        '-[packetsSent/s]',\n",
    "        '-[bytesSent_in_bits/s]',\n",
    "        '-frameWidth',\n",
    "        '-framesPerSecond',\n",
    "        '-totalPacketSendDelay',\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]',\n",
    "        '-qualityLimitationReason',\n",
    "        '-qualityLimitationResolutionChanges']\n",
    "    target_substrings_RIV = [\n",
    "        '-roundTripTime',\n",
    "        '-fractionLost']\n",
    "    target_substrings_RIA = [\n",
    "        '-fractionLost']\n",
    "    target_substrings_ROA = [\n",
    "        '-roundTripTime']\n",
    "    target_substrings_SV2 = [\n",
    "        '-width',\n",
    "        '-framesPerSecond']\n",
    "    target_substrings_AP = [\n",
    "        '-totalPlayoutDelay']\n",
    "    \n",
    "    #final dictionary data types to store all the values. \n",
    "    #each (None None None) triple will be filled with (values, start time, end time)\n",
    "    target_values_dict_IT01V = {\n",
    "        '-[packetsReceived/s]': (None, None, None),\n",
    "        '-packetsLost': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-totalFreezesDuration': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None),\n",
    "        '-totalProcessingDelay': (None, None, None),\n",
    "        '-jitter': (None, None, None)}\n",
    "    target_values_dict_IT01A = {\n",
    "        '-[bytesReceived_in_bits/s]': (None, None, None)}\n",
    "    target_values_dict_OT01V = {\n",
    "        '-[packetsSent/s]': (None, None, None),\n",
    "        '-[bytesSent_in_bits/s]': (None, None, None),\n",
    "        '-frameWidth': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None),\n",
    "        '-totalPacketSendDelay': (None, None, None),\n",
    "        '-[totalPacketSendDelay/packetsSent_in_ms]': (None, None, None),\n",
    "        '-qualityLimitationReason': (None, None, None),\n",
    "        '-qualityLimitationResolutionChanges': (None, None, None)}\n",
    "    target_values_dict_RIV = {\n",
    "        '-roundTripTime': (None, None, None),\n",
    "        '-fractionLost': (None, None, None)}\n",
    "    target_values_dict_RIA = {\n",
    "        '-fractionLost': (None, None, None)}\n",
    "    target_values_dict_ROA = {\n",
    "        '-roundTripTime': (None, None, None)}\n",
    "    target_values_dict_SV2 = {\n",
    "        '-width': (None, None, None),\n",
    "        '-framesPerSecond': (None, None, None)}\n",
    "    target_values_dict_AP = {\n",
    "        '-totalPlayoutDelay': (None, None, None)}\n",
    "    \n",
    "    #begin searching for the target statistics\n",
    "    for key, value in stats.items():\n",
    "        key_string = str(key)\n",
    "        \n",
    "        # inbound video ones\n",
    "        if key_string[:5] == 'IT01V': \n",
    "            for target_substring in target_substrings_IT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {}) #jump into the innermost dictionary\n",
    "                    if target_values_dict_IT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_IT01V[target_substring] = (info['values'], info['startTime'], info['endTime']) #just record whats in the values\n",
    "            #special case for finding jitter because it is a substring of other keys too\n",
    "            if key_string[-7:] == '-jitter':\n",
    "                info = stats.get(key, {})\n",
    "                if target_values_dict_IT01V['-jitter'] == (None, None, None):\n",
    "                    target_values_dict_IT01V['-jitter'] = (info['values'], info['startTime'], info['endTime'])\n",
    "        \n",
    "        # inbound audio ones\n",
    "        elif key_string[:5] == 'IT01A':\n",
    "            for target_substring in target_substrings_IT01A:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_IT01A[target_substring] == (None, None, None):\n",
    "                        target_values_dict_IT01A[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "    \n",
    "        # outbound video ones\n",
    "        elif key_string[:5] == 'OT01V':\n",
    "            for target_substring in target_substrings_OT01V:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_OT01V[target_substring] == (None, None, None):\n",
    "                        target_values_dict_OT01V[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "                    \n",
    "        # remote inbound video ones\n",
    "        elif key_string[:3] == 'RIV':\n",
    "            for target_substring in target_substrings_RIV:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIV[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIV[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "        \n",
    "        # remote inbound audio ones\n",
    "        elif key_string[:3] == 'RIA':\n",
    "            for target_substring in target_substrings_RIA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_RIA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_RIA[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "        \n",
    "        # remote outbound audio ones\n",
    "        elif key_string[:3] == 'ROA':\n",
    "            for target_substring in target_substrings_ROA:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_ROA[target_substring] == (None, None, None):\n",
    "                        target_values_dict_ROA[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "                    \n",
    "        # video source ones\n",
    "        elif key_string[:3] == 'SV2':\n",
    "            for target_substring in target_substrings_SV2:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_SV2[target_substring] == (None, None, None):\n",
    "                        target_values_dict_SV2[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "                    \n",
    "        # audio playout ones\n",
    "        elif key_string[:2] == 'AP':\n",
    "            for target_substring in target_substrings_AP:\n",
    "                if target_substring in key_string:\n",
    "                    info = stats.get(key, {})\n",
    "                    if target_values_dict_AP[target_substring] == (None, None, None):\n",
    "                        target_values_dict_AP[target_substring] = (info['values'], info['startTime'], info['endTime'])\n",
    "        \n",
    "    if verbose:    \n",
    "        print(\"\\n\\n---------------------------------INBOUND VIDEO STATS---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_IT01V.items():\n",
    "            print(key, \": \", value[0])\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------INBOUND AUDIO STATS---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_IT01A.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------OUTBOUND VIDEO STATS---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_OT01V.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------REMOTE INBOUND VIDEO---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_RIV.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------REMOTE INBOUND AUDIO---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_RIA.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------REMOTE OUTBOUND AUDIO---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_ROA.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------VIDEO SOURCE STATS---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_SV2.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n---------------------------------AUDIO PLAYOUT STATS---------------------------------\\n\")\n",
    "        for key, value in target_values_dict_AP.items():\n",
    "            print(key, \": \", value)\n",
    "            print(\"Start Time: \", value[1], \" |  End Time: \", value[2])\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "    IT01V_unique = {\n",
    "        'IT01V_packetsRecieved': target_values_dict_IT01V['-[packetsReceived/s]'],\n",
    "        'IT01V_packetsLost': target_values_dict_IT01V['-packetsLost'],\n",
    "        'IT01V_frameWidth': target_values_dict_IT01V['-frameWidth'],\n",
    "        'IT01V_totalFreezesDuration': target_values_dict_IT01V['-totalFreezesDuration'],\n",
    "        'IT01V_framesPerSecond': target_values_dict_IT01V['-framesPerSecond'],\n",
    "        'IT01V_bytesReceived_in_bits/s': target_values_dict_IT01V['-[bytesReceived_in_bits/s]'],\n",
    "        'IT01V_totalProcessingDelay': target_values_dict_IT01V['-totalProcessingDelay'],\n",
    "        'IT01V_jitter': target_values_dict_IT01V['-jitter']}\n",
    "    IT01A_unique = {\n",
    "        'IT01A_bytesReceived_in_bits/s': target_values_dict_IT01A['-[bytesReceived_in_bits/s]']}\n",
    "    OT01V_unique = {\n",
    "        'OT01V_packetsSent/s': target_values_dict_OT01V['-[packetsSent/s]'],\n",
    "        'OT01V_bytesSent_in_bits/s': target_values_dict_OT01V['-[bytesSent_in_bits/s]'],\n",
    "        'OT01V_frameWidth': target_values_dict_OT01V['-frameWidth'],\n",
    "        'OT01V_framesPerSecond': target_values_dict_OT01V['-framesPerSecond'],\n",
    "        'OT01V_totalPacketSendDelay': target_values_dict_OT01V['-totalPacketSendDelay'],\n",
    "        'OT01V_totalPacketSendDelay/packetsSent_in_ms': target_values_dict_OT01V['-[totalPacketSendDelay/packetsSent_in_ms]'],\n",
    "        'OT01V_qualityLimitationReason': target_values_dict_OT01V['-qualityLimitationReason'],\n",
    "        'OT01V_qualityLimitationResolutionChanges': target_values_dict_OT01V['-qualityLimitationResolutionChanges']}\n",
    "    RIV_unique = {\n",
    "        'RIV_roundTripTime': target_values_dict_RIV['-roundTripTime'],\n",
    "        'RIV_fractionLost': target_values_dict_RIV['-fractionLost']}\n",
    "    RIA_unique = {\n",
    "        'RIA_fractionLost': target_values_dict_RIA['-fractionLost']}\n",
    "    ROA_unique = {\n",
    "        'ROA_roundTripTime': target_values_dict_ROA['-roundTripTime']}\n",
    "    SV2_unique = {\n",
    "        'SV2_width': target_values_dict_SV2['-width'],\n",
    "        'SV2_framesPerSecond': target_values_dict_SV2['-framesPerSecond']}\n",
    "    AP_unique = {\n",
    "        'AP_totalPlayoutDelay': target_values_dict_AP['-totalPlayoutDelay']}\n",
    "        \n",
    "    return IT01V_unique, IT01A_unique, OT01V_unique, RIV_unique, RIA_unique, ROA_unique, SV2_unique, AP_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Functions which convert/manipulate parsed data ----------------\n",
    "\n",
    "def separate_by_comma(text_list):\n",
    "    '''Function which takes a list in text form and converts it to a proper python list'''\n",
    "    \n",
    "    temp = \"\"\n",
    "    list_list = []\n",
    "    for char in text_list:\n",
    "        if char == \"[\":\n",
    "            temp = \"\"\n",
    "        elif char == \"]\":\n",
    "            try:\n",
    "                list_list.append(float(temp))\n",
    "            except ValueError:\n",
    "                list_list.append(temp)\n",
    "        elif char == \",\":\n",
    "            try:\n",
    "                list_list.append(float(temp))\n",
    "            except ValueError:\n",
    "                list_list.append(temp)\n",
    "            temp = \"\"\n",
    "        else:\n",
    "            temp = temp + char\n",
    "    \n",
    "    return list_list\n",
    "\n",
    "\n",
    "def iso_to_unix_time(iso_string):\n",
    "    '''funtion converting ISO time (like in Web RTC) to unix time'''\n",
    "\n",
    "    dt = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    unix_time = int(dt.timestamp())\n",
    "    return unix_time\n",
    "\n",
    "\n",
    "def add_padding(data_dicts, global_start, global_end, verbose=False):\n",
    "    '''Function to add -1s to all empty time entries'''\n",
    "    \n",
    "    new_data_dicts = []\n",
    "    for dictionary in data_dicts:\n",
    "        \n",
    "        new_dictionary = {}\n",
    "        for key, value in dictionary.items():\n",
    "            start_time_unix = iso_to_unix_time(value[1])\n",
    "            end_time_unix = iso_to_unix_time(value[2])\n",
    "            start_padding = []\n",
    "            end_padding = []\n",
    "            \n",
    "            if start_time_unix > global_start:\n",
    "                start_padding = [-1] * (start_time_unix - global_start)\n",
    "            if end_time_unix < global_end:\n",
    "                end_padding = [-1] * (global_end - end_time_unix)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"gs =\", global_start, \"stu =\", start_time_unix, \"lenfp =\", len(start_padding))\n",
    "                print(\"ge =\", global_end, \"etu =\", end_time_unix, \"lenep =\", len(end_padding), \"\\n\")\n",
    "\n",
    "            new_values = start_padding + separate_by_comma(value[0]) + end_padding\n",
    "            new_dictionary[key] = new_values\n",
    "        \n",
    "        new_data_dicts.append(new_dictionary)\n",
    "    return new_data_dicts\n",
    "\n",
    "\n",
    "def align_times(data_dicts_ellen, data_dicts_aadya, verbose=False):\n",
    "    '''function which, for a single call, lines up all the times of the indivdual \n",
    "    stats by adding -1s for all seconds where there is no data.'''\n",
    "    \n",
    "    global_start = 999999999999999999999999\n",
    "    global_end = 0\n",
    "    \n",
    "    for dictionary in data_dicts_ellen:\n",
    "        for key, value in dictionary.items():\n",
    "            start_time_unix = iso_to_unix_time(value[1])\n",
    "            end_time_unix = iso_to_unix_time(value[2])\n",
    "            if start_time_unix < global_start:\n",
    "                global_start = start_time_unix\n",
    "            if end_time_unix > global_end:\n",
    "                global_end = end_time_unix\n",
    "    \n",
    "    for dictionary in data_dicts_aadya:\n",
    "        for key, value in dictionary.items():\n",
    "            start_time_unix = iso_to_unix_time(value[1])\n",
    "            end_time_unix = iso_to_unix_time(value[2])\n",
    "            if start_time_unix < global_start:\n",
    "                global_start = start_time_unix\n",
    "            if end_time_unix > global_end:\n",
    "                global_end = end_time_unix\n",
    "    \n",
    "    new_data_dicts_ellen = add_padding(data_dicts_ellen, global_start, global_end, verbose)\n",
    "    new_data_dicts_aadya = add_padding(data_dicts_aadya, global_start, global_end, verbose)\n",
    "    return new_data_dicts_ellen, new_data_dicts_aadya  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Functions which parse, then manipulate, then write data into a CSV ----------------\n",
    "\n",
    "def parse_all_treatments(parent_file_path, lowest_treatment_number, highest_treatment_number):\n",
    "\n",
    "    for treatment in range(lowest_treatment_number, highest_treatment_number + 1):\n",
    "        select_correct_parser(parent_file_path, treatment)\n",
    "\n",
    "    print(f\"Successfully parsed {highest_treatment_number - lowest_treatment_number + 1} calls.\")\n",
    "\n",
    "\n",
    "def select_correct_parser(parent_file_path, treatment_number, verbose=False):\n",
    "    '''Function which chooses the correct parser depending on which files exist.\n",
    "    \n",
    "    EVENTUALLY THIS SHOULD BE ADAPTED TO RECOGNISE DODGY FILES INSTEAD\n",
    "    \n",
    "    '''\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "\n",
    "    ellen_exists = os.path.exists(file_path_ellen)\n",
    "    aadya_exists = os.path.exists(file_path_aadya)\n",
    "\n",
    "    if ellen_exists and not aadya_exists:\n",
    "        return parse_and_convert_to_csv_single_treatment_ellen_only(parent_file_path, treatment_number, verbose)\n",
    "    elif aadya_exists and not ellen_exists:\n",
    "        return parse_and_convert_to_csv_single_treatment_aadya_only(parent_file_path, treatment_number, verbose)\n",
    "    elif ellen_exists and aadya_exists:\n",
    "        return parse_and_convert_to_csv_single_treatment(parent_file_path, treatment_number, verbose)\n",
    "    else:\n",
    "        print(f\"Treatment {treatment_number} had no acceptable data to parse\")\n",
    "\n",
    "\n",
    "def parse_and_convert_to_csv_single_treatment(parent_file_path, treatment_number, verbose=False):\n",
    "    '''function which creates a single CSV files for all the stats for a single call'''\n",
    "\n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\" #VERY IMPORTANT THAT FILE SUFFIXES\n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\" #CONFORM TO THIS CONVENTION!\n",
    "\n",
    "    Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap = get_stats(file_path_ellen)\n",
    "    Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap = get_stats(file_path_aadya)\n",
    "    \n",
    "    all_ellens_data_unaligned = [Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap]\n",
    "    all_aadyas_data_unaligned = [Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap]\n",
    "    all_ellens_data, all_aadyas_data = align_times(all_ellens_data_unaligned, all_aadyas_data_unaligned, verbose)\n",
    "    \n",
    "    #printing things if we want to:\n",
    "    if verbose:\n",
    "        for edict in range(len(all_ellens_data)):\n",
    "            for key, val in all_ellens_data[edict].items():\n",
    "                print(\"OG\", key, \":\", all_ellens_data_unaligned[edict][key][0])\n",
    "                print(\"Start Time:\", iso_to_unix_time(all_ellens_data_unaligned[edict][key][1]))\n",
    "                print(\"END Time:\", iso_to_unix_time(all_ellens_data_unaligned[edict][key][2]))\n",
    "                print(\"NEW\", key, \":\", val, \"\\n\")\n",
    "        print(\"\\n\\n\\n\")\n",
    "        for adict in range(len(all_aadyas_data)):\n",
    "            for key, val in all_aadyas_data[adict].items():\n",
    "                print(\"OG\", key, \":\", all_aadyas_data_unaligned[adict][key][0])\n",
    "                print(\"Start Time:\", iso_to_unix_time(all_aadyas_data_unaligned[adict][key][1]))\n",
    "                print(\"END Time:\", iso_to_unix_time(all_aadyas_data_unaligned[adict][key][2]))\n",
    "            print(\"NEW\", key, \":\", val, \"\\n\")\n",
    "\n",
    "    dict_names = [\"Inbound Video\", \"Inbound Audio\", \"Outbound Video\", \"Remote Inbound Video\", \"Remote Inbound Audio\", \"Remote Outbound Audio\", \"Source Video\", \"Audio Playback\"]\n",
    "    \n",
    "    matrix = []\n",
    "    for data_dict in range(len(all_ellens_data)):\n",
    "        for key, value_list in all_ellens_data[data_dict].items():\n",
    "            none = None\n",
    "    \n",
    "    output_file = f\"treatment{treatment_number}.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "\n",
    "        for data_dict in range(len(all_ellens_data)):\n",
    "            for key, value_list in all_ellens_data[data_dict].items():\n",
    "                # Create a row where the first element is the key, followed by the values\n",
    "                row1 = [\"Ellen_\" + key] + value_list\n",
    "                row2 = [\"Aadya_\" + key] + all_aadyas_data[data_dict][key]\n",
    "                # Write the rows to the CSV file\n",
    "                writer.writerow(row1)\n",
    "                writer.writerow(row2)\n",
    "\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "    \n",
    "    \n",
    "def parse_and_convert_to_csv_single_treatment_ellen_only(parent_file_path, treatment_number, verbose=False):\n",
    "    '''function which creates a single CSV with all of Ellen's computer's stats for \n",
    "    a single call.'''\n",
    "    \n",
    "    file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "    Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap = get_stats(file_path_ellen)\n",
    "    all_ellens_data_unaligned = [Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap]\n",
    "    all_ellens_data, extra = align_times(all_ellens_data_unaligned, [], verbose)\n",
    "    \n",
    "    output_file = f\"treatment{treatment_number}_ellen_only.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for data_dict in range(len(all_ellens_data)):\n",
    "            for key, value_list in all_ellens_data[data_dict].items():\n",
    "                # Create a row where the first element is the key, followed by the values\n",
    "                row1 = [\"Ellen_\" + key] + value_list\n",
    "                # Write the rows to the CSV file\n",
    "                writer.writerow(row1)\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "\n",
    "    \n",
    "def parse_and_convert_to_csv_single_treatment_aadya_only(parent_file_path, treatment_number, verbose=False):\n",
    "    '''function which creates a single CSV with all of Aadya's computer's stats for \n",
    "    a single call.'''\n",
    "    \n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "    Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap = get_stats(file_path_aadya)\n",
    "    all_aadyas_data_unaligned = [Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap]\n",
    "    extra, all_aadyas_data = align_times([], all_aadyas_data_unaligned, verbose)\n",
    "    \n",
    "    output_file = f\"treatment{treatment_number}_aadya_only.csv\"\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for data_dict in range(len(all_aadyas_data)):\n",
    "            for key, value_list in all_aadyas_data[data_dict].items():\n",
    "                # Create a row where the first element is the key, followed by the values\n",
    "                row2 = [\"Aadya_\" + key] + all_aadyas_data[data_dict][key]\n",
    "                # Write the rows to the CSV file\n",
    "                writer.writerow(row2)\n",
    "    print(f\"Data has been written to {output_file}\")\n",
    "\n",
    "\n",
    "#FUNCTION I HAVENT FINISHED YET BUT WOULD CREATE A SINGLE CSV FILE WITH EVERY STAT FROM EVERY CALL \n",
    "def parse_and_convert_into_csv_all_treatments(parent_file_path, total_number_of_treatments):\n",
    "    '''\n",
    "    output_file = \"all_stats_all_treatments_13Aug\"\n",
    "    dict_names = [\"Inbound Video\", \"Inbound Audio\", \"Outbound Video\", \"Remote Inbound Video\", \"Remote Inbound Audio\", \"Remote Outbound Audio\", \"Source Video\", \"Audio Playback\"]\n",
    "    \n",
    "    header = [\"Statistic\"]\n",
    "    for i in range(total_number_of_treatments):\n",
    "        header.append(f\"treatment{i}\")\n",
    "        \n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        file_path_ellen = parent_file_path + str(treatment_number) + \"_ellen.txt\"\n",
    "        all_ellens_data = [Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap]\n",
    "        \n",
    "    file_path_aadya = parent_file_path + str(treatment_number) + \"_aadya.txt\"\n",
    "\n",
    "    Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap = get_stats(file_path_ellen)\n",
    "    Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap = get_stats(file_path_aadya)\n",
    "    \n",
    "    all_ellens_data = [Eit01v, Eit01a, Eot01v, Eriv, Eria, Eroa, Esv2, Eap]\n",
    "    all_aadyas_data = [Ait01v, Ait01a, Aot01v, Ariv, Aria, Aroa, Asv2, Aap]\n",
    "    \n",
    "    for dictionary in range(len(all_ellens_data)):\n",
    "        for key, value in all_ellens_data[dictionary].items():\n",
    "            all_ellens_data[dictionary][key] = separate_by_comma(value)\n",
    "    for dictionary in range(len(all_aadyas_data)):\n",
    "        for key, value in all_aadyas_data[dictionary].items():\n",
    "            all_aadyas_data[dictionary][key] = separate_by_comma(value)\n",
    "    \n",
    "    dict_names = [\"Inbound Video\", \"Inbound Audio\", \"Outbound Video\", \"Remote Inbound Video\", \"Remote Inbound Audio\", \"Remote Outbound Audio\", \"Source Video\", \"Audio Playback\"]\n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to treatment1.csv\n",
      "Data has been written to treatment2.csv\n",
      "Data has been written to treatment3.csv\n",
      "Successfully parsed 3 calls.\n"
     ]
    }
   ],
   "source": [
    "#file paths for testing_9_Aug\n",
    "file_path_parent_01 = \"testing_stats/testing_9_Aug/treatment\"\n",
    "#file path parent for testing_13_Aug\n",
    "file_path_parent_02 = \"testing_stats/testing_13_Aug/treatment\" \n",
    "#file path parent for testing_27_Aug\n",
    "file_path_parent_03 = \"testing_stats/testing_27_Aug/treatment\"\n",
    "\n",
    "lowest_treatment_number = 1\n",
    "highest_treatment_number = 3\n",
    "parse_all_treatments(file_path_parent_03, lowest_treatment_number, highest_treatment_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
